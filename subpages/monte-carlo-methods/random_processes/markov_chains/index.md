---
title: Markov Chains
layout: default
noindex: true
---

# Markov Chains

In probability theory and statistics, a **Markov chain** or **Markov process** is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

### TODO: Add PAS2 source

### TODO: Add Wiki source