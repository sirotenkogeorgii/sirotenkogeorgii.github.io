---
layout: default
title: Randomized Algorithms
date: 2024-10-20
excerpt: ...
tags:
  - algorithms
  - probability
  - theory
---

# Randomized Algorithms

## Introduction to Randomized Strategies

Randomization is a powerful tool in algorithm design, often providing simpler or more efficient solutions to problems where deterministic approaches may fail or become overly complex. By introducing randomness, we can avoid deadlocks, improve average-case performance, and create secure protocols.

### Avoiding Deadlock in Communication

Consider two network nodes, $A$ and $B$, connected by a half-duplex channel. This channel allows communication in both directions, but only one node can send at any given time. If both nodes attempt to transmit simultaneously, they recognize the collision but cannot retrieve any data.

To prevent a permanent deadlock—where both nodes repeatedly try to start at the same moment—a randomized protocol can be implemented:

1. If one side is already transmitting, the other side must wait.
2. If both sides attempt to start at the same time, both must stop. Each node then waits for a random time interval before attempting to send again.

### Deterministic vs. Randomized Quicksort

The Quicksort algorithm sorts a list by choosing a pivot element and partitioning the list into elements smaller and larger than that pivot. The efficiency of this algorithm depends heavily on the quality of the pivot.

* Deterministic Quicksort: The pivot is chosen according to a fixed procedure (e.g., always picking the first element or the median of the first, last, and middle elements). In this version, rare "bad inputs" exist that will always trigger a degenerated partition tree, causing the complexity to spike to $O(n^2)$ instead of the average $O(n \log n)$.
* Randomized Quicksort: The pivot is chosen uniformly at random from the current list. Here, no specific input is "bad" by itself. While there is a small probability that the algorithm makes a series of poor random choices, the expected performance is robust across all possible inputs.

### The Copy Game and Strategic Randomization

Randomization is equally vital in game-theoretical contexts. Consider the copy game, where two players, A and B, secretly commit to a bit $x_A, x_B \in \lbrace 0, 1\rbrace$. Player A wins if the bits are distinct; Player B wins if they are identical.

If Player A uses a deterministic strategy that B can learn or simulate, B can simply duplicate A's value, causing A to lose every round. however, if Player A uses a randomized strategy—determining their bit by a fair coin toss independent of B—then B cannot gain an advantage. On average, each player wins half the rounds, regardless of B's computational power or intelligence.


## Boolean Functions and Cryptographic Applications

The parity function and the properties of the Exclusive-Or (XOR) operation form the basis for several fundamental randomized protocols.

### The Parity Function

An $n$-ary Boolean function maps $\lbrace 0, 1\rbrace^n \to \lbrace 0, 1\rbrace$, where 0 represents false and 1 represents true.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">($n$-ary Parity Function ($\oplus_n$))</span></p>

A Boolean function on $n$ arguments that yields 1 if and only if an odd number of the arguments are equal to 1.

</div>

The 2-ary parity function, or XOR ($\oplus$), is defined as

$$
a \oplus b =
\begin{cases}
1 & \text{if } a \neq b, \\
0 & \text{otherwise}.
\end{cases}
$$

The binary parity function is associative. In expressions containing only $\oplus$, parentheses do not affect the result; the expression will always evaluate to the parity of all arguments involved. For example, $(a_1 \oplus a_2) \oplus (a_1 \oplus (a_3 \oplus a_1))$ is equivalent to $\oplus_5(a_1, a_2, a_1, a_3, a_1)$.

### One-Time Pad

The one-time pad is a technique used to encrypt a message $w = w_1 \dots w_n$ using a secret random word $r = r_1 \dots r_n$, where $r$ is generated by independent fair coin tosses.

* Encryption: The sender computes $w \oplus r = (w_1 \oplus r_1) \dots (w_n \oplus r_n)$.
* Security: Without knowledge of $r$, an adversary $E$ cannot gain any information about $w$. Because $r$ is uniform and random, the encrypted message $w \oplus r$ is equally likely to be any word of length $n$.

**Note.** The random word $r$ must never be reused. If $r$ is used to encrypt both $w_0$ and $w_1$, an adversary can compute $(w_0 \oplus r) \oplus (w_1 \oplus r) = w_0 \oplus w_1$, revealing whether the two messages were identical.

### The Dining Cryptographers Problem

Three cryptographers (A, B, C) wish to determine if one of them paid their restaurant bill or if an external entity (like the NSA) paid. They require a protocol where:

1. Everyone learns if a cryptographer paid.
2. If a cryptographer paid, the identity of that person remains anonymous to the others.

### The Protocol

1. Every pair of cryptographers tosses a fair coin to establish a shared random bit unknown to the third party (e.g., A and B share $r_{A,B}$).
2. Each cryptographer $X$ calculates $u_X$, the parity of the two bits they know (e.g., $u_A = r_{A,B} \oplus r_{A,C}$).
3. The sum of these parities is always zero: $u_A \oplus u_B \oplus u_C = (r_{A,B} \oplus r_{A,C}) \oplus (r_{A,B} \oplus r_{B,C}) \oplus (r_{A,C} \oplus r_{B,C}) = 0$.
4. Each cryptographer publishes a bit $p_X$:
  * If they did not pay, $p_X = u_X$.
  * If they did pay, $p_X = \neg u_X$ (the complement).
5. The result $p_A \oplus p_B \oplus p_C$ will be 0 if no one paid and 1 if exactly one person paid.


## Foundations of Discrete Probability

Randomized algorithms are analyzed using the framework of discrete probability theory.

### Discrete Probability Spaces

A chance experiment is modeled using a set $\Omega$ of outcomes. $\Omega$ must be finite or countably infinite.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Discrete Probability Distribution)</span></p>

A real-valued function Prob: $\Omega \to [0, 1]$ such that: $\sum_{\omega \in \Omega} \mathbb{P}[\omega] = 1$. The pair $(\Omega, Prob)$ is called a discrete probability space.

</div>

* Events: Subsets of $\Omega$. The probability of an event $E$ is $\mathbb{P}[E] = \sum_{\omega \in E}\mathbb{P}[\omega]$.
* Atomic Events: Singleton subsets $\lbrace\omega\rbrace$, often identified simply as $\omega$.
* Uniform Distribution: On a finite set $\Omega$, this distribution assigns $\mathbb{P}[\omega] = \frac{1}{\lvert \Omega\rvert}$ to every outcome. Note that a uniform distribution cannot exist on a countably infinite set.

### Random Variables

A random variable $X$ is a function $X$: $\Omega \to \mathbb{R}$. It maps outcomes of a probability space to real numbers.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Distribution of a Random Variable)</span></p>

The range of $X$, denoted range($X$), is a countable subset of $\mathbb{R}$. The distribution $\mathbb{P}_X$ is defined as: $\mathbb{P}_X(r) = \mathbb{P}[X = r] = \sum_{\lbrace\omega \in \Omega: X(\omega) = r\rbrace}\mathbb{P}[\omega]$

</div>

### Indicator Variables

A specific type of random variable is the indicator variable for an event $E$:

$$
X(\omega) =
\begin{cases}
1 & \text{if } \omega \in E, \\
0 & \text{if } \omega \notin E.
\end{cases}
$$


## Joint Distributions and Independence

When dealing with multiple random variables $X_1, \dots, X_m$ on the same probability space, we must consider their collective behavior.

### Joint Distribution

The joint probability distribution maps combinations of values $(r_1, \dots, r_m)$ to the probability that all variables take those values simultaneously: $\mathbb{P}_{X_1, \dots, X_m}(r_1, \dots, r_m) = \mathbb{P}[X_1 = r_1, \dots, X_m = r_m]$ Crucially, the individual distributions of $X_i$ do not necessarily determine the joint distribution.

### Independence

Independence describes how the knowledge of one variable's value affects the probability of others.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Mutually Independent Random Variables)</span></p>

Variables $X_1, \dots, X_m$ are mutually independent if for all $r_1, \dots, r_m$: $\mathbb{P}[X_1 = r_1, \dots, X_m = r_m] = \mathbb{P}[X_1 = r_1] \cdot \dots \cdot \mathbb{P}[X_m = r_m]$

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Pairwise Independent Random Variables)</span></p>

Variables are pairwise independent if for all $i \neq j$: $\mathbb{P}[X_i = r_i, X_j = r_j] = \mathbb{P}[X_i = r_i] \cdot \mathbb{P}[X_j = r_j]$

</div>

### k-wise Independence

This concept generalizes pairwise independence. Variables are $k$-wise independent if every subset of $k$ distinct variables is mutually independent.

### Note on Independence Levels

* Mutual independence is equivalent to $m$-wise independence.
* $k'$-wise independence implies k-wise independence if $k < k'$.
* The reverse is false: variables can be pairwise independent without being triplewise (3-wise) independent.

<div class="math-callout math-callout--question" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Example</span><span class="math-callout__name">(Pairwise but Not Triplewise Independence)</span></p>

Consider three fair coin tosses $X_1, X_2, X_3$. Define:

</div>

* $Z_1 = X_1 \oplus X_2$
* $Z_2 = X_1 \oplus X_3$
* $Z_3 = X_2 \oplus X_3$ Here, any two $Z_i$ are independent. However, because $Z_1 = Z_2 \oplus Z_3$, the third variable is entirely determined by the first two. Thus, $\mathbb{P}[Z_1 = 1, Z_2 = 0, Z_3 = 0] = 0$, which is not equal to $\mathbb{P}[Z_1=1]\mathbb{P}[Z_2=0]\mathbb{P}[Z_3=0] = 1/8$.


## Expectation

The expectation (or expected value) is the long-run average value of a random variable.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Expectation)</span></p>

$\mathbb{E}[X] = \sum_{\omega \in \Omega}\mathbb{P}[\omega]X(\omega)$ This is defined only if the series converges absolutely.

</div>

### Properties of Expectation

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Linearity of Expectation)</span></p>

For any random variables $X_1, \dots, X_n$ and any real number $r$:

$$
\mathbb{E}[X_1 + \dots + X_n] = \mathbb{E}[X_1] + \dots + \mathbb{E}[X_n]
$$

$$
\mathbb{E}[rX] = r \mathbb{E}[X]
$$

</div>

If the variables $X_1, \dots, X_n$ are mutually independent, the expectation of their product also decomposes: $\mathbb{E}[X_1 \cdot \dots \cdot X_n] = \mathbb{E}[X_1] \cdot \dots \cdot \mathbb{E}[X_n]$

<div class="math-callout math-callout--question" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Example</span><span class="math-callout__name"></span></p>

Fixed Points of a Random Permutation Suppose $n$ gifts are distributed to $n$ people via a random permutation $\pi$. Let $X_i$ be an indicator variable that is 1 if person $P_i$ receives their own gift $T_i$. The probability $\mathbb{P}[X_i = 1] = \frac{1}{n}$. The expected number of people who get their own gift is: $\mathbb{E}\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n \mathbb{E}[X_i] = \sum_{i=1}^n \frac{1}{n} = 1$

</div>

### Conditional Expectation

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Conditional Distribution and Expectation)</span></p>

For an event $F$ where $\mathbb{P}[F] > 0$: $\mathbb{P}[X = r \mid F] = \frac{\mathbb{P}[\lbrace X = r\rbrace \cap F]}{\mathbb{P}[F]}\mathbb{E}[X \mid F] = \sum_{r \in range(X)} r \cdot\mathbb{P}[X = r \mid F]$

</div>


## Probabilistic Bounds

Bounds allow us to estimate the probability of "bad" events even when the exact distribution is complex.

### Markov Inequality

This is a simple "tail bound" for non-negative random variables.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Markov Inequality)</span></p>

Let $X$ be a non-negative random variable. For any $r > 0$: $\mathbb{P}[X \ge r]\le\frac{\mathbb{E}[X]}{r}$

</div>

**Proof.**

1. By the definition of expectation: $\mathbb{E}[X] = \sum_{\omega \in \Omega}\mathbb{P}[\omega]X(\omega)$.
2. Since $X$ is non-negative, we can truncate the sum to only those $\omega$ where $X(\omega) \ge r$: $\mathbb{E}[X]\ge\sum_{\lbrace\omega: X(\omega) \ge r\rbrace}\mathbb{P}[\omega]X(\omega)$
3. In this truncated sum, every $X(\omega)$ is at least $r$: $\mathbb{E}[X]\ge r \sum_{\lbrace\omega: X(\omega) \ge r\rbrace}\mathbb{P}[\omega]$
4. The sum $\sum_{\lbrace\omega: X(\omega) \ge r\rbrace}\mathbb{P}[\omega]$ is exactly $\mathbb{P}[X \ge r]$.
5. Thus, $\mathbb{E}[X]\ge r\cdot\mathbb{P}[X \ge r]$, which rearranges to the stated inequality. $\square$

### The Sum Bound

The sum bound (also known as the Union Bound) states that the probability of at least one error event occurring is at most the sum of the individual error probabilities: $\mathbb{P}[E_1 \cup \dots \cup E_n]\le\sum_{i=1}^n\mathbb{P}[E_i]$ This bound is powerful because it requires no assumptions about the independence of the events $E_i$.

### The Repetition Bound

If a single experiment has a success probability of at least $\frac{1}{n}$, we can increase the likelihood of success through independent repetitions.

#### If we repeat the experiment $tn$ times

1. The probability of failure in one trial is $\le 1 - \frac{1}{n}$.
2. The probability of failure in tn independent trials is $\le (1 - \frac{1}{n})^{tn} = ((1 - \frac{1}{n})^n)^t$.
3. Using the fact that $(1 - \frac{1}{n})^n\to\frac{1}{e} < 0.4$ as $n\to\infty$: $\mathbb{P}[\text{no success}]\le 0.4^t\le\frac{1}{2^t}$

Consequently, the probability of at least one success in $n$ repetitions is at least $\frac{1}{2}$, and in tn repetitions, it is at least $1 - \frac{1}{2^t}$.



## Fundamental Probability Bounds and the Tenure Game

In the study of randomized algorithms, we often need to bound the probability that a random variable deviates significantly from its expectation or that a sequence of independent trials fails to produce a desired outcome. This section introduces essential tail bounds and applies them to a combinatorial game known as the Tenure Game to demonstrate how probabilistic reasoning can reveal deterministic winning strategies.

### Fundamental Probability Bounds

Analyzing randomized systems requires a toolkit of inequalities to estimate the likelihood of specific events.

### Markov's Inequality

The most basic tool for bounding the "tail" of a distribution (the probability that a variable is much larger than its mean) is Markov's Inequality. It applies to any non-negative random variable.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Markov's Inequality)</span></p>

Let X be a random variable that assumes only non-negative values. Then for every positive real number $r$, we have

$$
\mathbb{P}[X \ge r] \le \frac{\mathbb{E}[X]}{r}.
$$

</div>

**Proof.** Let $(\Omega, \mathbb{P})$ be the probability space on which $X$ is defined. We derive the bound as follows:

1. By definition: $\mathbb{E}[X] = \sum_{\omega \in \Omega} \mathbb{P}[\omega] X(\omega)$.
2. Since $X$ is non-negative, we can restrict to outcomes with $X(\omega) \ge r$:
   $\mathbb{E}[X] \ge \sum_{\lbrace\omega \in \Omega : X(\omega) \ge r\rbrace} \mathbb{P}[\omega] X(\omega)$.
3. Each summand in this restricted sum is at least $r$ times its probability:
   $\mathbb{E}[X] \ge r \sum_{\lbrace\omega \in \Omega : X(\omega) \ge r\rbrace} \mathbb{P}[\omega]$.
4. The remaining sum equals $\mathbb{P}[X \ge r]$.
5. Therefore, $\mathbb{E}[X] \ge r \cdot \mathbb{P}[X \ge r]$, hence $\mathbb{P}[X \ge r] \le \frac{\mathbb{E}[X]}{r}$.

### The Sum Bound

The Sum Bound (often referred to as the Union Bound) provides an upper limit on the probability that at least one of several "error" events occurs.

The Sum Bound Suppose the only way a chance experiment may fail is if the outcome is one of $n$ possible error events $E_1, \dots, E_n$, where $E_i$ occurs with probability $\mathbb{P}[E_i]$. Then the probability of failure is at most the sum of the individual error probabilities:

$$
\mathbb{P}[\text{Failure}] \le \sum_{i=1}^n \mathbb{P}[E_i].
$$

**Note.** This bound is powerful because it requires no assumptions about the relationship between events; specifically, the events $E_i$ do not need to be mutually independent.

### The Repetition Bound

When an experiment has a low probability of success, we can increase the overall success probability through independent trials.

Consider an experiment with a success probability of at least $1/n$. If we repeat this experiment $tn$ times independently (where $t$ is a natural number):

* The probability of failure in a single trial is at most $1 - 1/n$.
* The probability of no success in $tn$ repetitions is $(1 - 1/n)^{tn}$.

Using the calculus identity where $(1 - 1/n)^n$ approaches $1/e < 0.4$ as $n \to $\infty$ (where $e \approx 2.718$ is Euler's number): $\left(1 - \frac{1}{n}\right)^{tn} = \left(\left(1 - \frac{1}{n}\right)^n\right)^t \leq 0.4^t \leq \frac{1}{2^t}$

Thus, the probability of at least one success in $tn$ independent repetitions is at least $1 - 1/2^t$. Specifically, if $t=1$ (i.e., we run $n$ repetitions), the probability of at least one success is at least $1/2$.


## The Tenure Game

The Tenure Game is a finite, two-person, zero-sum game with complete information. It serves as an excellent model for applying probabilistic analysis to determine the existence of winning strategies.

### Game Rules and Mechanics

The game involves two players: Alice (the department head, who wants to prevent tenure) and Bob (the dean, who wants at least one person to receive tenure).

1. Initial Configuration: Finitely many tokens $1, \dots, m$ are placed at positions $d_1, \dots, d_m$, where each $d_i$ is a non-zero natural number. Position 0 represents "tenure."
2. The Rounds: In each round:
  * Step I (Partition): Bob partitions the set $I$ of tokens currently not at position 0 into two disjoint sets, $I_0$ and $I_1$.
  * Step II (Selection): Alice chooses a bit $r \in \lbrace 0, 1\rbrace$.
  * Step III (Removal and Promotion): Tokens in the set $I_r$ are removed from the game. Tokens in the set $I_{1-r}$ are moved one step closer to position 0 (e.g., a token at position $d$ moves to $d-1$).
3. Winning Condition: Bob wins if eventually any token reaches position 0. Alice wins if all tokens are removed before any reaches position 0.

### Potential and Randomized Analysis

To analyze who has a winning strategy, we define the potential of a configuration.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Potential)</span></p>

For a configuration with m tokens at positions $d_1, \dots, d_m$, the potential $\Phi$ is defined as: $\Phi = \sum_{i=1}^m \frac{1}{2^{d_i}}$

</div>

We can understand this potential by considering a randomized strategy for Alice: Suppose Alice chooses the bit $r$ in each round by tossing a fair coin.

* Each token $i$ at position $d_i$ must survive $d_i$ consecutive rounds to reach position 0.
* Since Alice's coin tosses are independent and uniform, the probability that token $i$ survives and reaches 0 is exactly $(1/2)^{d_i}$.
* Let $X_i$ be an indicator variable where $X_i = 1$ if token $i$ reaches 0, and $X_i = 0$ otherwise. Then $\mathbb{E}[X_i] = \frac{1}{2^{d_i}}$.
* The total number of tokens reaching tenure is $X = X_1 + \dots + X_m$. By linearity of expectation, $\mathbb{E}[X] = \sum \frac{1}{2^{d_i}} = \Phi$.

### Determining the Winner

The value of the initial potential $\Phi$ determines which player has a winning strategy.

1. Alice's Winning Condition ($\Phi < 1$) If the initial potential is strictly less than 1, Alice has a winning strategy.

* Probabilistic Argument: If $\mathbb{E}[X] < 1$, there must exist at least one sequence of coin tosses where $X < 1$. Since $X$ must be an integer, $X < 1$ implies $X = 0$. Thus, there is a sequence of moves where Alice wins.
* Deterministic Strategy: Alice can maintain the invariant that $\Phi < 1$. When Bob partitions $I$ into $I_0$ and $I_1$, the potential $\Phi = \Phi_0 + \Phi_1$. Alice chooses to remove the part with the higher potential. By choosing $r$ such that $\Phi_r \geq \Phi_{1-r}$, she ensures that the potential of the remaining, promoted tokens stays below 1.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Alice's Winning Condition)</span></p>

If the potential of the initial configuration is strictly less than 1, then Alice has a winning strategy.

</div>

2. Bob's Winning Condition $(\Phi \geq 1)$ If the initial potential is at least 1, Bob has a winning strategy.

* Bob's Strategy: Bob must partition the tokens such that both $I_0$ and $I_1$ have a potential of at least $1/2$. If he does this, no matter which part Alice removes, the remaining part (when promoted one step closer to 0) will have its potential doubled, resulting in a new potential $\Phi' = 2 \cdot (\text{potential } \geq 1/2) \geq 1$.
* Feasibility: Bob can always find such a partition.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Lemma</span><span class="math-callout__name">(Splitting Potential at $1/2$)</span></p>

Let $d_1 \leq \dots \leq d_m$ be a non-decreasing sequence of non-zero natural numbers where $\sum_{i=1}^m \frac{1}{2^{d_i}} \geq \frac{1}{2}$. Then there exists an index $t$ such that: $\sum_{i=1}^t \frac{1}{2^{d_i}} = \frac{1}{2}$.

</div>

**Proof.** Let t be the minimum index such that the partial sum $s_t = \sum_{i=1}^t \frac{1}{2^{d_i}} \geq \frac{1}{2}$.

1. Assume for contradiction that $s_t > 1/2$.
2. Because $d_1 \leq \dots \leq d_t$, all terms $\frac{1}{2^{d_1}}, \dots, \frac{1}{2^{d_t}}$ are multiples of $\frac{1}{2^{d_t}}$. Thus, both $s_t$ and $\frac{1}{2}$ are multiples of $\frac{1}{2^{d_t}}$.
3. By the minimality of $t$, $s_t - 1/2^{d_t} < 1/2$.
4. However, if $s_t > 1/2$ and both are multiples of $1/2^{d_t}$, the smallest possible value for $s_t$ is $1/2 + 1/2^{d_t}$.
5. This implies $s_t - \frac{1}{2^{d_t}} \geq \frac{1}{2}$, which contradicts the minimality of $t$. Thus, $s_t$ must equal $1/2$.


## Excursus: Game Theory Foundations

The analysis of the Tenure Game relies on general principles of game theory concerning finite games with perfect information.

### Definitions and Game Trees

In a finite two-person game with perfect information, players take turns making moves with full knowledge of all previous moves. The game must end in a finite number of steps with a winner and a loser.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Strategy)</span></p>

A strategy for a player is a function that determines their next move for every possible configuration where it is their turn. A winning strategy is one that guarantees a win regardless of the opponent's moves.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Determined Game)</span></p>

A game is determined if one of the two players possesses a winning strategy.

</div>

To visualize these games, we use a game tree:

* Nodes: Represent configurations.
* Root: The initial configuration.
* Edges: Represent admissible moves to children nodes.
* Leaves: Terminal configurations where a winner is decided.

### König's Lemma and Determinacy

The finiteness of these games is often supported by König's Lemma.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Lemma</span><span class="math-callout__name">(König's Lemma)</span></p>

König's Lemma Any finitely branching infinite tree has an infinite path.

</div>

**Proof.**

1. Start at the root $v_0$. Since the tree is infinite and the root has finitely many children, at least one child must be the root of an infinite subtree.
2. Pick that child as $v_1$.
3. Repeat this process inductively. Because each node $v_i$ has an infinite subtree and only finitely many children, there will always be at least one child $v_{i+1}$ that also maintains an infinite subtree. This construct generates an infinite path $v_0$, $v_1$, $\dots$.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Finite Games are Determined)</span></p>

Finite Games are Determined Every finite two-person game with perfect information is determined.

</div>

**Proof.** The proof proceeds by induction on the height of the game tree.

1. Base Case: If the height is 0, the node is a leaf, and the winner is already decided.
2. Inductive Step: For a node of height $h > 0$, assume Player A moves. By the inductive hypothesis, all children nodes (subgames of height $< h$) are determined. If there is at least one child where Player A has a winning strategy, then Player A has a winning strategy for the current node. If all children are winning for Player B, then Player B has a winning strategy for the current node.

<div class="math-callout math-callout--remark" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Remark</span><span class="math-callout__name"></span></p>

This determinacy does not necessarily hold for infinite games. Using the axiom of choice, one can define sets of sequences (winning conditions) for which neither player has a winning strategy in an infinite game.

</div>


## Randomized Algorithms: Graph Cuts and Derandomization Techniques

This document explores randomized strategies for solving graph-theoretic problems, specifically focusing on finding maximum cuts and independent sets. It details the analysis of these algorithms through the lens of expectation and introduces two powerful methods for derandomization: the Method of Conditional Expectations and the use of Small Sample Spaces through $k$-wise independence.

### Randomized Algorithms for Finding a Cut

In the context of this study, a graph refers to a finite undirected graph $G = (V, E)$.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Cut)</span></p>

A cut of a graph $G = (V, E)$ is a partition of the vertex set $V$ into two disjoint subsets $V_0$ and $V_1$.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Weight of a Cut)</span></p>

The weight of a cut $(V_0, V_1)$ is the number of edges between a node in $V_0$ and a node in $V_1$. Formally,

$$\left\|\lbrace\lbraceu, v\rbrace \in E : u \in V_0, v \in V_1\rbrace\right\|$$

</div>

### Algorithm Cut

The Algorithm Cut is a simple randomized approach to find a cut in a graph $G = (V, E)$ where $V = \lbrace 1, \dots, n\rbrace$:

1. Choose random bits $r_1, \dots, r_n$ by tosses of a fair coin.
2. Assign nodes to partitions: $V_0 = \lbrace i : r_i = 0\rbrace$ and $V_1 = \lbrace i : r_i = 1\rbrace$.
3. Output the cut $(V_0, V_1)$.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Expected Weight of a Cut)</span></p>

Let $G$ be a graph with $m$ edges. On input $G$, the expected weight of the cut returned by Algorithm Cut is $\frac{m}{2}$. Consequently, every graph $G$ possesses a cut with a weight of at least $\frac{m}{2}$.

</div>

**Proof.** Let $G = (V, E)$ have edges $e_1, \dots, e_m$. We introduce indicator variables $\hat{e}_i$, where $\hat{e}_i = 1$ if edge $e_i = \lbrace u_i, v_i\rbrace$ crosses the cut, and $\hat{e}_i = 0$ otherwise. The total weight $\hat{w}_G$ of the cut is the sum of these indicators: $\hat{w}_G = \sum_{i=1}^m \hat{e}_i$. By the linearity of expectation: $\mathbb{E}[\hat{w}_G] = \mathbb{E}\left[\sum_{i=1}^m \hat{e}_i\right] = \sum_{i=1}^m \mathbb{E}[\hat{e}_i]$. To find $\mathbb{E}[\hat{e}_i]$, we calculate the probability that the edge $e_i = \lbrace u_i, v_i\rbrace$ crosses the cut: $\mathbb{E}[\hat{e}_i] = 1 \cdot \mathbb{P}(\hat{e}_i = 1) + 0 \cdot \mathbb{P}(\hat{e}_i = 0) = \mathbb{P}(u_i \in V_0 \text{ and } v_i \in V_1) + \mathbb{P}(u_i \in V_1 \text{ and } v_i \in V_0)$. Because node assignments are mutually (and thus pairwise) independent: $\mathbb{E}[\hat{e}_i] = \mathbb{P}(u_i \in V_0)\mathbb{P}(v_i \in V_1) + \mathbb{P}(u_i \in V_1)\mathbb{P}(v_i \in V_0) = \left(\frac{1}{2} \cdot \frac{1}{2}\right) + \left(\frac{1}{2} \cdot \frac{1}{2}\right) = \frac{1}{2}$. Therefore, $\mathbb{E}[\hat{w}_G] = \sum_{i=1}^m \frac{1}{2} = \frac{m}{2}$. If every possible cut had a weight strictly less than $m/2$, the expected value would also be strictly less than $m/2$. Since the expectation is exactly $m/2$, there must exist at least one sequence of coin tosses yielding a cut weight $\geq m/2$.


## Derandomization by Conditional Expectation

Derandomization is the process of transforming a randomized algorithm into a deterministic one. The Method of Conditional Expectations achieves this by making choices one bit at a time to ensure the expected outcome does not decrease.

Let $\alpha$ be a binary word of length $s \le n$ representing the first $s$ random bits. Let $Cut_\alpha$ be the algorithm where $r_1, \dots, r_s$ are fixed to $\alpha$, and $r_{s+1}, \dots, r_n$ remain random. Let $\mathbb{E}[\hat{w}_G(\alpha)]$ be the expected weight of the cut produced by $Cut_\alpha$.

We can observe that: $\mathbb{E}[\hat{w}_G(\alpha)] = \frac{1}{2}\mathbb{E}[\hat{w}_G(\alpha 0)] + \frac{1}{2}\mathbb{E}[\hat{w}_G(\alpha 1)]$ This implies that at least one of the choices ($r_{s+1} = 0$ or $r_{s+1} = 1$) must yield an expected value $\ge\mathbb{E}[\hat{w}_G(\alpha)]$. By starting with the empty word $\lambda$ (where $\mathbb{E}[\hat{w}_G(\lambda)] = m/2$) and greedily picking the bit that maximizes the conditional expectation, we can guarantee a final weight $w_G \ge m/2$.

### Derandomized Algorithm Cut (Conditional Expectation)

1. Input: Graph $G = (V, E)$.
2. For $s = 1, \dots, n$:
  * If $\mathbb{E}[\hat{w}_G(r_1 \dots r_{s-1}0)] \ge \mathbb{E}[\hat{w}_G(r_1 \dots r_{s-1}1)]$, set $r_s = 0$.
  * Else, set $r_s = 1$.
3. Output: Cut $(V_0, V_1)$ defined by $r_1, \dots, r_n$.

### Efficiency of the Derandomized Version

To evaluate the condition efficiently, we partition the edges $E$ into sets based on the current step $s$:

* $E_1$: Edges where both endpoints are $< s$ (already determined).
* $E_2$: Edges where at least one endpoint is $> s$ (assignment still random).
* $E_3^i$: Edges $\lbrace j, s\rbrace$ where $j < s$ and $r_j = i$.

The choice of $r_s$ affects the edges in $E_3^i$. Specifically, the expected weight is: $\mathbb{E}[\hat{w}_G(r_1 \dots r_{s-1}r_s)] = \lvert E'_1\rvert + \frac{1}{2}\lvert E_2\rvert + \lvert E_3^{1-r_s}\rvert$ where $E'_1$ are edges in $E_1$ that already cross the cut. To maximize this value, we choose $r_s$ such that we maximize the number of edges crossing between node s and previously placed nodes.

Deterministic Rule: Set $r_s = 0$ if the number of neighbors of s already in $V_1$ is greater than or equal to the number of neighbors in $V_0$. Otherwise, set $r_s = 1$.


## Derandomization by Small Sample Spaces

Another approach to derandomization involves reducing the size of the sample space (the set of all possible random bit strings) and then exhaustively searching it (trivial derandomization).

Trivial Derandomization: Simulating a randomized algorithm for every possible value of its random source. If an algorithm uses $k$ random bits, this takes $2^k$ simulations. If $k = O(\log n)$, the derandomized algorithm runs in polynomial time.

### Pairwise Independence

The analysis of Algorithm Cut only requires the bits $r_1, \dots, r_n$ to be pairwise independent and uniformly distributed. We can generate $n$ pairwise independent bits using only $t = \lceil\log(n+1) \rceil$ truly random bits.

### Construction of n Pairwise Independent Bits

1. Let $t = \lceil \log(n+1) \rceil$ and $I = \lbrace 1, \dots, t\rbrace$.
2. Choose $b_1, \dots, b_t$ using a fair coin.
3. Let $J_1, \dots, J_n$ be pairwise distinct non-empty subsets of $I$.
4. For $i = 1, \dots, n$, set $r_i = \bigoplus_{j \in J_i}b_j$ (the parity/XOR of the selected bits).

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Lemma</span><span class="math-callout__name">(Verification of Construction)</span></p>

Verification of Construction The random bits $r_1, \dots, r_n$ obtained by the XOR construction are pairwise independent and each bit is uniformly distributed.

</div>

**Proof.**

1. Uniform Distribution: For any non-empty $J \subseteq\lbrace 1, \dots, t\rbrace$, let $s \in J$. Then $r = (\bigoplus_{j \in J \setminus \lbrace s\rbrace}b_j) \oplus b_s$. Since $b_s$ is a fair coin flip and independent of other bits, $r$ is equally likely to be 0 or 1.
2. Pairwise Independence: Consider two distinct non-empty sets $J$ and $J'$. There must be an index $s$ that is in one set but not the other (assume $s \in J$, $s \notin J'$).
  * $r = (\bigoplus_{j \in J \setminus \lbrace s\rbrace} b_j) \oplus b_s$
  * $r' = \bigoplus_{j \in J'} b_j$ Since $s \notin J'$, $r'$ is independent of $b_s$. For any fixed value of $r'$, the bit $b_s$ ensures $r$ is 0 or 1 with probability $1/2$. Thus $\mathbb{P}(r=u \mid r'=v) = 1/2$, proving $\mathbb{P}(r=u \land r'=v) = \mathbb{P}(r=u)\mathbb{P}(r'=v)$.

### Derandomized Algorithm Cut (Pairwise Independence)

Because the sample space size is $2^t \approx n$, we can check all possible assignments of $b_1, \dots, b_t$ in polynomial time:

1. Input: Graph $G = (V, E)$.
2. Set $t = \lceil\log(n+1)\rceil$ and $m = 2^t$.
3. For each of the $m$ possible binary strings $b$ of length $t$:
  * Generate $r_{b,j}$ for all nodes $j=1 \dots n$ using the XOR construction.
  * Calculate the weight of the resulting cut.
4. Output: The cut with the maximum weight among all $m$ checked.


## Derandomization of Algorithm HyperCut

A hypergraph $G = (V, E)$ consists of nodes $V$ and hyperedges $E$, which are subsets of $V$. A hypergraph is $k$-regular if every hyperedge contains exactly $k$ nodes. A cut $(V_0, V_1)$ has a weight equal to the number of hyperedges intersecting both $V_0$ and $V_1$.

### Algorithm HyperCut

1. Assign each node $i$ a random bit $r_i \in \lbrace 0, 1\rbrace$ via fair coin tosses.
2. Output cut $(V_0, V_1)$ based on these bits.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Expected Weight in Hypergraphs)</span></p>

For a $k$-regular hypergraph with $m$ edges, the expected weight of the cut is $(1 - \frac{2}{2^k})m$.

</div>

**Proof.** A hyperedge $e_i$ of size $k$ does not cross the cut only if all its nodes are in $V_0$ or all are in $V_1$. The probability of this is $\frac{1}{2^k} + \frac{1}{2^k} = \frac{2}{2^k}$. Thus, the probability an edge crosses is $1 - \frac{2}{2^k}$. By linearity of expectation, $\mathbb{E}[\hat{w}_G] = (1 - \frac{2}{2^k})m$.

### Derandomization via $k$-wise Independence

The analysis of HyperCut requires $k$-wise independence: any subset of $k$ variables must be mutually independent. We can construct $k$-wise independent bits using algebraic methods over a finite field.

### Construction via Polynomials

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">($k$-wise Independent Numbers)</span></p>

Let $p$ be a prime number. Let $\hat{a}_0, \dots, \hat{a}_{k-1}$ be chosen uniformly and independently from $\lbrace 0, \dots, p-1\rbrace$. For $i = 1, \dots, p$, define $r_i$ as: $r_i = \hat{a}_{k-1}i^{k-1} + \hat{a}_{k-2}i^{k-2} + \dots + \hat{a}_0 \pmod p$. Then $r_1, \dots, r_p$ are uniformly distributed and $k$-wise independent.

</div>

**Proof.**

* Uniformity: For any $r_i$, if coefficients $\hat{a}_1, \dots, \hat{a}_{k-1}$ are fixed, the $p$ possible values of $\hat{a}_0$ result in $p$ distinct values for $r_i$.
* Independence: For any $k$ distinct indices $i_1, \dots, i_k$ and target values $b_1, \dots, b_k$, we look for a solution to the system:

  $$
  \begin{pmatrix}
    i_1^{k-1} & \dots & 1 \\
    \vdots & \ddots & \vdots \\
    i_k^{k-1} & \dots & 1
  \end{pmatrix}
  \begin{pmatrix}
    a_{k-1} \\
    \vdots \\
    a_0
  \end{pmatrix}
  =
  \begin{pmatrix}
    b_1 \\
    \vdots \\
    b_k
  \end{pmatrix}
  $$

  This matrix is a Vandermonde matrix. Its determinant is $\prod_{1 \le j < j' \le k} (i_{j'} - i_j)$. Since $i_j$ are distinct, the determinant is non-zero, the matrix is invertible, and a unique solution for the coefficients exists. The probability of picking these specific coefficients is $1/p^k$, satisfying the requirement for $k$-wise independence.


## Independent Sets in Hypergraphs

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Independent Set)</span></p>

In a hypergraph $G = (V, E)$, a subset $U \subseteq V$ is independent if it does not contain any hyperedge $e \in E$.

</div>

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Size of Independent Sets)</span></p>

A $3$-regular hypergraph with $n$ nodes and $m \ge n/3$ hyperedges has an independent set $U$ where: $\lvert U\rvert \ge \frac{n\sqrt{n}}{3\sqrt{m}}$

</div>

### Algorithm IndependentSet

1. Input: Hypergraph $G$, parameter $p \in [0, 1]$.
2. Choose random bits $r_i$ such that $\mathbb{P}(r_i = 1) = p$ (biased coin).
3. Let $T = \lbrace i : r_i = 1\rbrace$ be the initial candidate set.
4. Identify "bad" edges: $D = \lbrace e \in E : e \subseteq T\rbrace$.
5. Create $Y$ by picking the minimum node from each edge in $D$.
6. Output $U = T \setminus$ Y.

### Analysis of Algorithm IndependentSet

The set $U$ is guaranteed to be independent because for every hyperedge originally in $T$, at least one node was removed.

* $\mathbb{E}[\lvert T\rvert] = np$.
* Let $\hat{e}$ be an indicator that edge $e$ is in $T$. $\mathbb{E}[\hat{e}] = \mathbb{P}(e \subseteq T) = p^3$ (since hyperedges have $3$ nodes).
* $\mathbb{E}[\lvert D\rvert] = \sum$ $\mathbb{E}[\hat{e}] = mp^3$.
* Since $\lvert Y\rvert \le \lvert D\rvert$, we have $\mathbb{E}[\lvert U\rvert] = \mathbb{E}[\lvert T\rvert] - \mathbb{E}[\lvert Y\rvert] \ge np - mp^3$.

To maximize $f(p) = np - mp^3$, we find the derivative $n - 3mp^2$ and set it to 0. This gives $p = \sqrt{\frac{n}{3m}}$. Substituting $p$ back into the expectation formula:

$\mathbb{E}[\lvert U\rvert] \ge n\left(\sqrt{\frac{n}{3m}}\right) - m\left(\sqrt{\frac{n}{3m}}\right)^3 = \frac{n\sqrt{n}}{\sqrt{3m}} - \frac{n\sqrt{n}}{3\sqrt{3m}} = \frac{2}{3\sqrt{3}}\frac{n\sqrt{n}}{\sqrt{m}} \ge \frac{1}{3}\frac{n\sqrt{n}}{\sqrt{m}}$

**Note.** This algorithm can be derandomized using the Method of Conditional Expectations (maintaining the potential function $\mathbb{E}[\lvert T\rvert] - \mathbb{E}[\lvert D\rvert]$) or Small Sample Spaces (using 3-wise independent biased variables).



## Byzantine Agreement

The Byzantine agreement problem explores how a network of $n$ processors can reach a consensus on a single binary value $b$, even when a subset of those processors acts maliciously to prevent such an agreement. This problem is framed through the analogy of Byzantine generals attempting to coordinate an attack while dealing with potential traitors in their ranks.

In this model, we assume that at most a fraction of $\frac{1}{8}$ of all processors are bad (malicious). These bad processors can collaborate, share information, and deviate from the protocol in any way necessary to thwart the honest (good) processors.

### Problem Definition and Admissibility

Each processor starts with an initial binary value. The objective is for all good processors to eventually decide on a single value that satisfies specific conditions of consistency and validity.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Admissible Agreement)</span></p>

An agreement is considered admissible if it satisfies the following two conditions:

1. Agreement: All good processors must eventually agree on the same binary value $b \in\lbrace 0, 1\rbrace$.
2. Validity: If all good processors share the same initial value $v$, then the agreed-upon value $b$ must be equal to $v$.

</div>

### The Rules of Communication

To solve this problem, processors communicate in discrete rounds. The environment is governed by several strict rules:

* Round-Based Messaging: At the start of each round, every processor sends one message to every other processor.
* Targeted Deception: A bad processor is not required to send the same message to everyone; it may send different values to different receivers to create confusion.
* Information Asymmetry: Good processors do not know which other processors are bad, nor do they know the adversaries' strategies. A good processor only sees the messages it sends and receives.
* Adversarial Power: Bad processors have significant advantages. They can see all messages sent in the network, know the initial bits of all processors, and understand the strategy used by good processors. They may possess arbitrary computational power and can coordinate their messages at the start of each round to maximize disruption.

The rationale for assuming such a powerful, collaborative adversary is that any protocol capable of surviving a concerted, strategic attack is virtually guaranteed to succeed against random, non-malicious failures.


## The Randomized Byzantine Agreement Protocol

The following protocol, ByzantineAgreement, provides a randomized solution for a processor $i$ to reach a decision.

### Protocol Constants and Initialization

The protocol relies on two specific thresholds based on the total number of processors $n$:

* $t_0 = \frac{5}{8}$n
* $t_1 = \frac{6}{8}$n

Initially, each processor $i$ sets its internal value for the first round to its input bit: $v_i(1) = b_i$.

Execution Steps (For each round $s = 1, 2, \dots$)

1. Communication: Processor $i$ sends its current value $v_i(s)$ to all other processors. It then receives values $v_j(s)$ from all other processors $j$.
2. Counting: The processor counts the occurrences of each bit received. Let $c_0$ be the count of 0s and $c_1$ be the count of 1s.
3. Majority Selection:
  * If $c_0\ge c_1$, the processor sets its preferred value $u(s) = 0$ and its count $c(s) = c_0$.
  * Otherwise, it sets $u(s) = 1$ and $c(s) = c_1$.
4. Randomized Thresholding: The processors use a fair coin toss to obtain a random bit $\tau(s)\in\lbrace 0, 1\rbrace$. Crucially, this random bit is the same for all processors in a given round.
  * If $c(s) \ge t_{\tau(s)}$, then the processor updates its value for the next round: $v_i(s+1) = u(s)$.
  * Otherwise, it sets $v_i(s+1) = 0$ (the default value).
5. Termination: If $c(s) \ge \frac{7}{8}n$, the processor assumes an agreement has been reached on $u(s)$ and terminates.


## Analysis of the Protocol

The efficiency and correctness of the randomized approach are summarized in the following proposition:

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Efficiency of Randomized Byzantine Agreement)</span></p>

In a group of $n$ processors where at most $\frac{n}{8}$ processors are malicious and the remainder follow the ByzantineAgreement protocol:

</div>

1. An admissible agreement is reached with probability $1$.
2. The expected number of rounds to reach this agreement is constant.

**Note.** Comparison with Deterministic Protocols In the same environment, deterministic protocols are significantly less efficient. The following bounds are established for deterministic solutions:

* Lower Bound: Any deterministic protocol requires at least $\frac{n}{8} + 1$ rounds in the worst case.
* Upper Bound: There exists a deterministic protocol that can reach agreement in exactly $\frac{n}{8} + 1$ rounds.

### Proof of Correctness and Termination

The proof of the proposition is built upon three primary claims.

**Claim I (Initial Unanimity).** If all good processors start with the same initial value, they will reach an agreement at the end of the first round.

**Proof.** If all good processors have the initial value $b$, they all send $b$ in the first round. Since there are at least $\frac{7}{8}n$ good processors, every good processor will find that $u(1) = b$ and $c(1)\ge\frac{7}{8}n$. Consequently, every good processor assumes an agreement on $b$ immediately.

**Claim II (Propagation of Agreement).** If $s$ is the first round where any good processor assumes an agreement on value $b$, then all good processors will assume an agreement on that same value $b$ by the end of round $s+1$.

**Proof.**

1. Suppose a processor assumes agreement on $b$ in round $s$. This implies its count for $b$ was $c_b = c(s) \ge \frac{7}{8}n$.
2. Even if all $\frac{1}{8}n$ bad processors sent a different value, at least $\frac{6}{8}n$ good processors must have sent $b$ in round $s$.
3. Because so many good processors sent $b$, in round $s+1$, every good processor will receive $b$ from at least those $\frac{6}{8}n$ processors.
4. This ensures that by the end of round $s+1$, the counts for $b$ at every good processor will be high enough ( $\ge\frac{7}{8}n$) to trigger the termination condition for the same value $b$.

**Claim III (Probabilistic Convergence).** If no good processor has assumed an agreement before round $s$, there is a probability of at least $\frac{1}{2}$ that some processor will assume an agreement by the end of round $s+1$.

**Proof.** To prove this, we examine the distribution of bits sent by good processors in round $s$. Let $k_0$ and $k_1$ be the number of good processors sending 0 and 1, respectively, and let $k = \max\lbrace k_0, k_1\rbrace$. We consider two cases based on $k$:

* Case A ($k < \frac{5}{8}n$): In this scenario, both $k_0$ and $k_1$ are less than $\frac{5}{8}n$. Because the number of bad processors is at most $\frac{1}{8}n$, no processor can receive a count $c(s) \ge \frac{6}{8}n$. With probability $\frac{1}{2}$, the random bit $\tau(s)$ will be $1$, making the threshold $t_1 = \frac{6}{8}n$. Since no processor meets this threshold, all good processors will revert to the default value $0$ for round $s+1$. This creates unanimity, and agreement is reached.
* Case B ($k \ge\frac{5}{8}n$): Let $b$ be the bit such that $k_b\ge\frac{5}{8}n$. Regardless of what the bad processors do, every processor will receive at least $\frac{5}{8}n$ messages containing bit $b$. With probability $\frac{1}{2}$, the random bit $\tau(s)$ will be $0$, making the threshold $t_0 = \frac{5}{8}n$. In this case, all good processors will see $c(s) \ge t_0$ and will set their next value $v_i(s+1) = b$. Again, this results in unanimity for the next round.

In both cases, there is a $\frac{1}{2}$ chance that the random coin $\tau(s)$ forces all good processors to send the same bit in the next round, leading to agreement.

### Final Summary of the Proof

If there is initial unanimity, agreement is reached in round 1 (Claim I). If not, as long as agreement hasn't been reached, each round has at least a $\frac{1}{2}$ probability of forcing unanimity (Claim III). Once unanimity is forced or any one good processor reaches the termination threshold, all others follow in the subsequent round (Claim II). This results in a geometric distribution for the number of rounds, leading to a constant expected time for convergence.


## Randomized Algorithms: Stable Marriages and the Proposal Algorithm

In the study of combinatorial optimization and algorithmic game theory, the Stable Marriage Problem serves as a foundational model for understanding how distinct groups with individual preferences can be matched. This chapter explores the formal definition of stable marriages, the algorithm used to find them, and a rigorous analysis of that algorithm's efficiency using probabilistic tools.

### The Stable Marriage Problem

The problem begins with two disjoint sets of equal size $n$: a set of women $\lbrace F_1, \dots, F_n\rbrace$ and a set of men $\lbrace M_1, \dots, M_n\rbrace$. Every individual maintains a preference list—a strict ordering of all members of the opposite sex.

For instance, a woman $F_j$ prefers man $M_k$ over man $M_l$ if $k <_{F_j} l$, where $<_{F_j}$ denotes her specific ranking. Similar strict orderings exist for every man $M_i$. The objective is to pair every man with exactly one woman in a way that the resulting social structure is "stable."

### Formal Definitions

To analyze this problem mathematically, we must first define the structure of these pairings and the conditions that make them unstable.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Marriage)</span></p>

Given two sets of size $n$, a marriage is a bijection $\pi$ of the set $\lbrace 1, \dots, n\rbrace$. We identify a marriage $H$ with the set of binary relations

$$
H = \lbrace (F_1, M_{\pi(1)}), \dots, (F_n, M_{\pi(n)})\rbrace.
$$

</div>

A marriage is simply a complete matching. However, a marriage is not necessarily "stable" if individuals have the incentive to leave their assigned partners for someone else.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Stability and Dissatisfied Couples)</span></p>

A marriage $H$ is unstable if there exist two couples $(F_i, M_k)$ and $(F_j, M_l)$ in $H$ such that:

1. $l <_{F_i} k$ (Woman $F_i$ prefers man $M_l$ over her current partner $M_k$).
2. $i <_{M_l} j$ (Man $M_l$ prefers woman $F_i$ over his current partner $F_j$).

</div>

In this scenario, the pair $(F_i, M_l)$ is called dissatisfied. A marriage is stable if and only if it contains no dissatisfied couples.

Essentially, a marriage is unstable if there is a man and a woman who are not married to each other, but both would prefer each other over their current partners.

### The Proposal Algorithm

The existence of a stable marriage for any set of preference lists is guaranteed by the Marriage Theorem. This is proven constructively through the Proposal Algorithm (often known as the Gale-Shapley algorithm).

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(The Marriage Theorem)</span></p>

The Marriage Theorem Let $<_{F_1}, \dots, <_{F_n}$ and $<_{M_1}, \dots, <_{M_n}$ be strict orderings on $\lbrace 1, \dots, n\rbrace$. Then there always exists at least one stable marriage with respect to these orderings.

</div>

The following algorithm computes such a marriage by allowing men to "propose" to women based on their preferences.

### Algorithm Description

1. Initialize the set of marriages $H = \emptyset$.
2. While there is an unmarried man: a. Let $M_k$ be the minimum index of an unmarried man. b. Let $F_i$ be the most preferred woman in $M_k$'s list to whom he has not yet proposed. c. If $F_i$ is currently unmarried: - Match ($F_i$, $M_k$) and add to $H$. d. Else if $F_i$ is currently married to $M_l$: - If $M_k <_{F_i} M_l$ (she prefers $M_k$ over $M_l$): - $F_i$ "divorces" $M_l$. - Match $(F_i, M_k)$, and $M_l$ becomes unmarried. - Else: - $M_k$ remains unmarried (the proposal is rejected).
3. Output the stable marriage $H$.

### Verification of the Algorithm

The validity of the Proposal Algorithm rests on its termination and the stability of its output.

**Proof.**

1. Termination: In each iteration, a man proposes to a woman he has never approached before. Since there are $n$ men and $n$ women, there are at most $n^2$ possible unique proposals. Therefore, the loop must terminate within $n^2$ iterations.
2. Completeness: If the algorithm terminates while a man $M_k$ is unmarried, it implies he has proposed to every woman and been rejected by all. However, a woman only rejects a man if she is already matched or finds a better partner. Once a woman is married, she stays married (though her partner may change). If $M_k$ were unmarried, all $n$ women would have to be married to the other $n-1$ men, which is a mathematical impossibility. Thus, everyone must be married upon termination.
3. Stability: Suppose for contradiction that the output $H$ is unstable, containing couples $(F_i, M_k)$ and $(F_j, M_l)$ where $F_i$ and $M_l$ prefer each other. Because $M_l$ prefers $F_i$ to his final partner $F_j$, he must have proposed to $F_i$ before $F_j$. If $F_i$ is not married to $M_l$ in the end, she must have either rejected him or divorced him for someone she preferred more. Since a woman's partner's desirability only increases throughout the algorithm, her final partner $M_k$ must be at least as desirable as the man she preferred over $M_l$. This contradicts the assumption that she prefers $M_l$ to $M_k$.

### Properties of the Set of Stable Marriages

A single instance of the stable marriage problem may have multiple stable outcomes. We can compare these outcomes using a partial ordering.

### The Men's Preference Ordering

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Men's Preference Ordering)</span></p>

We define a relation $\le_M$ on the set of stable marriages. For two stable marriages $H$ and $H'$, $H\le_M H'$ if and only if every man prefers his wife in $H$ at least as much as (or the same as) his wife in $H'$.

</div>

This relation $\le_M$ is a partial ordering (reflexive, transitive, and antisymmetric).

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Possible Partners)</span></p>

A woman $F$ is considered possible for a man $M$ if there exists at least one stable marriage where $M$ and $F$ are paired.

</div>

The Proposal Algorithm is not neutral; it specifically favors the group making the proposals.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Optimality of the Proposal Algorithm)</span></p>

The marriage $H$ produced by the Proposal Algorithm is the least element with respect to the $\le_M$ relation. This means every man is married to the best possible woman available to him in any stable marriage.

</div>

**Proof.** We use induction on the sequence of proposals. We aim to show that no woman who rejects or divorces a man $M$ in the algorithm could ever be married to $M$ in any stable marriage.

* Base Case: The first rejection occurs. Suppose $F$ rejects $M$ because she prefers her current partner $M'$. If there were a stable marriage where $M$ is paired with $F$, then $M'$ must be paired with some woman $F'$ whom he likes less than $F$ (since $F$ is his first choice among non-rejected women). This would make $(F, M')$ a dissatisfied couple, proving $F$ was never possible for $M$.
* Inductive Step: Following the same logic, if $M$ is rejected by $F$ in favor of $M'$, and we assume all previous rejections were valid (the women were not "possible"), any marriage pairing $M$ and $F$ would result in $M'$ being paired with a woman he likes less than $F$, creating instability.

<div class="math-callout math-callout--remark" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Remark</span><span class="math-callout__name"></span></p>

Conversely, the marriage that is "best" for the men is the "worst" for the women. The result of the Proposal Algorithm is the greatest stable marriage with respect to the women's preference relation $\le_F$. Every woman receives her least desirable possible partner.

</div>

### Average-Case Complexity Analysis

While the worst-case complexity of the algorithm is $O(n^2)$, we are often interested in the average-case complexity: the expected number of proposals when preference lists are chosen uniformly at random.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Average-Case Complexity)</span></p>

The average number of proposals made by the Proposal Algorithm on inputs of order $n$ is $O(n \log n)$.

</div>

### The Proof via the Principle of Deferred Decisions

To prove this, we simplify the probabilistic model using three steps.

Step 1: Replacing Random Inputs with Random Choice By the Principle of Deferred Decisions, we do not need to fix the men's preference lists in advance. Instead, we can assume that whenever a man $M_k$ needs to propose, he chooses a woman $F_i$ uniformly at random from the set of women he hasn't proposed to yet. The expected number of proposals remains identical.

Step 2: Transition to the Amnesiac Version We consider an "amnesiac" version of the algorithm where a man chooses a woman uniformly at random from the set of all $n$ women, potentially proposing to the same woman multiple times.

* If he proposes to a woman who has already rejected or divorced him, he will certainly be rejected again.
* Therefore, the amnesiac version will always take at least as many proposals as the standard version. An upper bound on the amnesiac version serves as an upper bound for the standard algorithm.

Step 3: Anonymizing Men and the Coupon Collector's Problem In the amnesiac version, every proposal is a random draw from $n$ women. The algorithm terminates once every woman has received at least one proposal (because at that point, every woman will be married, and thus every man will be married). This is exactly the Coupon Collector's Problem.

### Mathematical Excursus: The Coupon Collector's Problem

To complete the complexity analysis, we must evaluate the expected time to "collect" all $n$ women.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Geometric Distribution)</span></p>

A random variable $X$ follows a geometric distribution with probability $p$ if $\mathbb{P}[X = i] = (1-p)^{i-1}p$ for $i > 0$. The expected value is $\mathbb{E}[X] = \frac{1}{p}$.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Harmonic Numbers)</span></p>

The $n$-th harmonic number is $H_n = \sum_{i=1}^n \frac{1}{i}$. It is bounded by $\ln n \le H_n \le \ln n + 1$.

</div>

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Expected Waiting Time of the Coupon Collector)</span></p>

In the coupon collector's problem with $n$ types, the expected number of rounds to collect all types is $nH_n$, which is $O(n \log n)$.

</div>

**Proof.** Let $X$ be the total number of proposals. Let $X_i$ be the number of proposals needed to find the $i$-th "new" woman after $i-1$ women have already been proposed to. The probability of hitting a new woman is $p_i = \frac{n - (i-1)}{n}$. Each $X_i$ is geometrically distributed with $\mathbb{E}[X_i] = \frac{1}{p_i} = \frac{n}{n-i+1}$. The total expected proposals $\mathbb{E}[X]$ is the sum of these expectations: $\mathbb{E}[X] = \sum_{i=1}^n \frac{n}{n-i+1} = n\left(\frac{1}{n} + \frac{1}{n-1} + \dots + 1\right) = nH_n$. Using the bound $H_n \approx \ln n$, we conclude $\mathbb{E}[X] = O(n \log n)$. This confirms that, on average, the Proposal Algorithm is highly efficient.



## Learning Probably Approximately Correct (PAC)

In the study of computational learning theory, we seek to understand how a system can learn an unknown subset of a base set $E$. This unknown subset is referred to as the target concept, denoted as $K_T$. The target concept is selected from a specific concept class $\mathcal{K}$, which is a collection of subsets of $E$ (i.e., $\mathcal{K} \subseteq 2^E$).

The process of learning involves receiving data about the target concept in the form of pairs: $(x_1, K_T(x_1)), \dots, (x_s, K_T(x_s))$

#### In this context

* The sample points or examples $x_j$ are selected randomly and independently from $E$ according to a probability distribution $\mu$ on $E$.
* The bit $K_T(x_j)$ indicates membership: it is 1 if $x_j$ is a member of the target concept and 0 otherwise.

A learner for a concept class $\mathcal{K}$ is an algorithm or mapping that takes this data and outputs a candidate concept $K_A \in \mathcal{K}$. The objective is for the candidate concept $K_A$ to closely approximate the target concept $K_T$.

### The PAC Framework

The discrepancy between the candidate concept and the target concept is measured using the probability distribution $\mu$. Specifically, for a given precision parameter $\epsilon > 0$, we require that the measure of the symmetric difference between $K_A$ and $K_T$ is small: $\mu(K_T \Delta K_A) \leq \epsilon$

It is natural to measure discrepancy relative to $\mu$ because it is intuitively difficult to approximate a concept in regions of $E$ that have a very small measure, as those regions are unlikely to be represented in the random sample $x_1, \dots, x_s$.

A learner must succeed in reaching this precision $\epsilon$ with a probability of at least $1 - \delta$, where $\delta > 0$ is the error bound. For a deterministic learner, this success probability refers to the randomness inherent in the choice of the sample $x_1, \dots, x_s$.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(PAC Learner)</span></p>

Let $E$ be any set and let $\mathcal{K}$ be a subset of the power set $2^E$ of $E$. The learner $A$ learns the concept class $\mathcal{K}$ probably approximately correct (PAC-learns) $\mathcal{K}$ with sample complexity $s$ if:

</div>

1. For every probability distribution $\mu$ on $E$,
2. For all $\epsilon, \delta > 0$ and for $s = s(\epsilon, \delta)$,
3. For all $K_T \in \mathcal{K}$,

If $x_1, \dots, x_s$ are randomly chosen from $E$ such that the $x_j$ are mutually independent and distributed according to $\mu$, then on the data $(x_1, K_T(x_1)), \dots, (x_s, K_T(x_s))$, the learner $A$ outputs a concept $K_A \in \mathcal{K}$ such that: $\mathbb{P}[\mu(K_T \Delta K_A) \leq \epsilon] \geq 1 - \delta$

### Variants of PAC Learning

While the core definition focuses on the probability-theoretical and combinatorial aspects, more restricted versions of PAC learning exist. These versions may require:

* The function mapping data to a candidate concept must be computable in polynomial time.
* The sample complexity $s = s(\epsilon, \delta)$ must be polynomial in the reciprocals of $\epsilon$ and $\delta$ (i.e., $s = p(1/\epsilon, 1/\delta)$ for some polynomial $p$).


## Case Study: PAC Learning of Axis-Parallel Rectangles

Consider the task of learning axis-parallel rectangles in the Euclidean plane $E = \mathbb{R}^2$. The concept class $\mathcal{K}$ consists of all rectangles of the form: $R(a, b, c, d) = \lbrace (x, y) \in \mathbb{R}^2 : a \leq x \leq b \text{ and } c \leq y \leq d \rbrace$ This class includes degenerate rectangles, such as line segments or the empty set.

### The Learner Algorithm

We define a learner $A$ that, given data $((x_1, y_1), b_1), \dots, ((x_s, y_s), b_s)$, identifies the sets of coordinates for all positive examples:

* $X = \lbrace x_i : b_i = 1 \rbrace$
* $Y = \lbrace y_i : b_i = 1 \rbrace$

The learner outputs the least axis-parallel rectangle that contains all positive examples: $R_A = R(\min X, \max X, \min Y, \max Y)$ If $X$ is empty, $R_A$ is the empty set. By construction, $R_A \subseteq R_T$ (the target rectangle), and $R_A$ is consistent with the data.

### Sample Complexity Analysis

To show $A$ is a PAC learner, we must determine the required sample complexity $s$. Fix $\mu$, $\epsilon$, $\delta$, and $R_T$.

Case I: $\mu(R_T)\leq\epsilon$ Since $R_A \subseteq R_T$, the symmetric difference $R_T \Delta R_A = R_T \setminus R_A \subseteq R_T$. Therefore, $\mu(R_T \Delta R_A)\leq\mu(R_T)\leq\epsilon$. In this case, the learner always succeeds regardless of the sample.

Case II: $\mu(R_T) > \epsilon$ We define four marginal strips ($R_1$ for right, $R_2$ for left, $R_3$ for top, and $R_4$ for bottom) within $R_T$, each having a measure of approximately $\epsilon/4$. Let:

* $z_1 = \sup\lbrace z : \mu(R(z, b, c, d)) \geq \epsilon/4\rbrace$
* $z_2 = \inf\lbrace z : \mu(R(a, z, c, d)) \geq \epsilon/4\rbrace$
* $z_3 = \sup\lbrace z : \mu(R(a, b, z, d)) \geq \epsilon/4\rbrace$
* $z_4 = \inf\lbrace z : \mu(R(a, b, c, z)) \geq \epsilon/4\rbrace$

We define $R_1 = R(z_1, b, c, d)$, $R_2 = R(a, z_2, c, d)$, $R_3 = R(a, b, z_3, d)$, and $R_4 = R(a, b, c, z_4)$. Let $R_{-i}$ be the same strips excluding their boundary edges such that $\mu(R_{-i}) \leq \epsilon/4$. Define $R_0 = R_T \setminus (R_{-1} \cup R_{-2} \cup R_{-3} \cup R_{-4})$.

If the sample contains at least one point from each strip $R_1$, $R_2$, $R_3$, $R_4$, then $R_0 \subseteq R_A \subseteq R_T$. The error is then: $\mu(R_T \Delta R_A) = \mu(R_T \setminus R_A)\subseteq\mu(R_T \setminus R_0)\leq\sum\mu(R_{-i})\leq 4(\epsilon/4) = \epsilon$

### Proof of Sample Complexity

1. For each $i \in\lbrace 1, 2, 3, 4\rbrace$, the probability that a single example misses strip $R_i$ is at most $(1 - \epsilon/4)$.
2. The probability that among $s$ examples, none are in $R_i$ is at most $(1 - \epsilon/4)^s$.
3. By the union bound, the probability that any of the four strips is missed is at most $4(1 - \epsilon/4)^s$.
4. We require $4(1 - \epsilon/4)^s\leq\delta$.
5. Solving for $s: s \log(1 - \epsilon/4) \leq \log(\delta/4) s \geq \frac{\log(4/\delta)}{-\log(1 - \epsilon/4)}$
6. Using the inequality $\epsilon/4 < -\log(1 - \epsilon/4)$, the learner succeeds if: $s \geq \frac{2 + \log(1/\delta)}{\epsilon/4}$


## PAC Learning of Finite Concept Classes

A key property of learners is their consistency with the provided data.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Consistent Learner)</span></p>

A concept $K$ is consistent with data $(x_1, b_1), \dots, (x_s, b_s)$ if $K(x_j) = b_j$ for all $j=1, \dots, s$. A learner for a concept class $\mathcal{K}$ is consistent if it always yields a candidate concept consistent with the given data.

</div>

**Note.** A learner is consistent if and only if the symmetric difference of the candidate and target concepts never contains an example from the provided data.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Finite Concept Class PAC Learning)</span></p>

Let $E$ be a set and let $\mathcal{K} \subseteq 2^E$ be a finite concept class. Every consistent learner for $\mathcal{K}$ is a PAC learner for $\mathcal{K}$ with sample complexity: $s = s(\epsilon, \delta) = \frac{\log \lvert \mathcal{K} \rvert + \log(1/\delta)}{\epsilon}$

</div>

**Proof.**

* It suffices to show that the probability the learner outputs a "bad" concept $K$ (where $\mu(K_T \Delta K) \geq \epsilon$) is at most $\delta$.
* A concept $K$ is consistent with the data for $K_T$ if and only if no sample points fall in the symmetric difference $K_T \Delta K$.
* For any "bad" concept $K$, the probability that a single sample point is not in $K_T \Delta K$ is at most $(1 - \epsilon)$.
* The probability that $K$ is consistent with all $s$ independent samples is at most $(1 - \epsilon)^s$.
* There are at most $\lvert\mathcal{K}\rvert$ such concepts. By the union bound, the probability that any bad concept is consistent with the data is at most $\lvert T\rvert\mathcal{K}\rvert(1 - \epsilon)^s$.
* We set $\lvert\mathcal{K}\rvert(1 - \epsilon)^s \leq \delta$. Using the fact that $(1 - \epsilon) \leq e^{-\epsilon}$, we find that $s \geq \frac{\ln\lvert\mathcal{K}\rvert + \ln(1/\delta)}{\epsilon}$ satisfies the condition.


## PAC Learning and VC Dimension

The concept of PAC learnability can be extended to infinite concept classes by using the Vapnik-Chervonenkis (VC) dimension.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(VC Dimension)</span></p>

A finite set $X \subseteq E$ is shattered by a concept class $\mathcal{K}$ if every possible subset of $X$ can be formed by the intersection of $X$ with some $K\in\mathcal{K}$ (i.e., $\lbrace K \cap X : K \in \mathcal{K} \rbrace = 2^X$).

</div>

The VC dimension of $\mathcal{K}$, denoted $\text{VC}(\mathcal{K})$, is the maximum size of a finite set $X$ that is shattered by $\mathcal{K}$.

### Examples of VC Dimension

* For a finite set $E$ and $\mathcal{K} = $2^E$, $\text{VC}(\mathcal{K}) = \lvert E\rvert$.
* For $E = \mathbb{R}$ and $\mathcal{K}$ as the set of finite intervals, $\text{VC}(\mathcal{K}) = 2$.
* For axis-parallel rectangles in $\mathbb{R}^2$, $\text{VC}(\mathcal{K}) = 4$.

<div class="math-callout math-callout--remark" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Remark</span><span class="math-callout__name"></span></p>

VC Dimension of Rectangles To see why $\text{VC}(\mathcal{K}) = 4$ for axis-parallel rectangles:

</div>

1. A set of four points (e.g., $(0, 1), (1, 0), (-1, 0), (0, -1)$) can be shattered.
2. No set of five points can be shattered. If we have five points, consider the bounding box formed by the minimum and maximum $x$ and $y$ coordinates. At most four points are needed to define these boundaries. Any rectangle containing these four "extreme" points must necessarily contain the fifth point if it lies within the bounding box. Thus, we cannot pick the four extreme points without also picking the fifth.

The existence of a finite VC dimension guarantees PAC learnability.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(VC Dimension and PAC Learning)</span></p>

Let $E$ be a set and $\mathcal{K} \subseteq 2^E$ be a concept class with $\text{VC}(\mathcal{K}) = d$. Every consistent learner for $\mathcal{K}$ is a PAC learner with sample complexity: $s = s(\epsilon, \delta) = O\left(\frac{d \log \frac{1}{\min\lbrace\epsilon, \delta\rbrace}}{\epsilon}\right)$. The constant in the $O$-notation is independent of $\mathcal{K}$, $\epsilon$, and $\delta$.

</div>


### Randomized Algorithms and Yao's Minimax Principle

In the study of algorithms, complexity is typically measured relative to the size of the input, denoted as $n$. This size might represent the length of a binary word, the number of nodes in a graph, or the elements in a list. When analyzing complexity, we often distinguish between worst-case complexity (the maximum resource usage on any input of size $n$) and average-case complexity (the expected resource usage averaged over all possible inputs of size $n$). These metrics can apply to various resources, such as running time, storage space, or the number of specific operations like comparisons.

The complexity of a problem is defined by the gap between its lower and upper bounds. An upper bound is established by constructing a specific algorithm and proving its complexity is at most a certain value. A lower bound is established by proving that no possible algorithm can solve the problem with a complexity lower than that value, often using combinatorial arguments. When these bounds match, the algorithm providing the upper bound is considered optimal.

### Las Vegas and Monte Carlo Algorithms

Randomized algorithms introduce elements of chance into their execution. They are generally categorized into two types:

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Las Vegas Algorithm)</span></p>

An algorithm that is always correct, regardless of the random choices made during execution. The randomness only influences the resources used (e.g., running time).

</div>

<div class="math-callout math-callout--question" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Example</span><span class="math-callout__name"></span></p>

Randomized Quicksort is a Las Vegas algorithm. It always produces a correctly sorted list, but its running time depends on the random selection of pivot elements.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Monte Carlo Algorithm)</span></p>

An algorithm that may produce an incorrect result with some probability.

</div>

<div class="math-callout math-callout--question" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Example</span><span class="math-callout__name"></span></p>

Integration by randomized sampling is a Monte Carlo algorithm. It estimates the area of a geometric figure by picking points uniformly at random; the result is an approximation that may deviate from the true value.

</div>

This chapter focuses on the expected complexity of Las Vegas algorithms in the worst case—that is, the expected resource usage on the "hardest" possible input of a given size.

### The Formal Framework for Yao’s Minimax Principle

To analyze the performance of randomized algorithms, we must first establish a formal setup for deterministic algorithms and inputs. For each input size $n$:

* $I_n$ is a finite set of all possible inputs of size $n$.
* $A_n$ is a finite set of all correct deterministic algorithms for inputs in $I_n$.
* $k_n$ : $A \times I\to\mathbb{N}$ is a cost function, where $k_n(A, I)$ represents the cost of running algorithm $A$ on input $I$.

This framework is particularly useful for black-box algorithms.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Black-Box Algorithm)</span></p>

An algorithm that can only access its input through specific types of queries. The algorithm cannot exploit information about the internal representation of the input beyond the results of these queries.

</div>

<div class="math-callout math-callout--question" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Example</span><span class="math-callout__name"></span></p>

Black-Box Sorting

</div>

In sorting, we assume the input is a list $x_1, \dots, x_n$ of distinct elements.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Black-Box Sorting Algorithm)</span></p>

A sorting algorithm where the input list can only be accessed by queries of the form "is $x_i < $_j$?".

</div>

#### For these algorithms

* $I_n$ is the set of all permutations of $\lbrace 1, \dots, n\rbrace$.
* $A_n$ is the set of all deterministic black-box algorithms that correctly sort the list (this set is finite if we exclude algorithms that repeat queries).
* $k(A, I)$ is the number of comparisons (queries) made.

### Representing Las Vegas Algorithms as Distributions

A Las Vegas algorithm can be viewed as a probability distribution over a set of deterministic algorithms. If an algorithm uses $r(n)$ random bits, each fixed sequence of those bits results in the algorithm behaving like a specific deterministic algorithm $A \in $A_n$.

Thus, for any size $n$, a Las Vegas algorithm is characterized by a probability distribution $\sigma_n$ on $A_n$, where $\mathbb{P}_{\sigma_n}[A]$ is the probability that the randomized algorithm behaves like deterministic algorithm $A$. Conversely, any distribution $\sigma_n$ on $A_n$ defines a randomized algorithm $A_{\sigma_n}$ that first selects a deterministic algorithm according to $\sigma_n$ and then executes it.

<div class="math-callout math-callout--remark" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Remark</span><span class="math-callout__name"></span></p>

Any sequence of probability distributions $\sigma_0, \sigma_1, \dots$ on $A_0, A_1, \dots$ represents a generalized Las Vegas algorithm. While these need not be "effective" (computable), they are valid for deriving lower bounds.

</div>

The expected cost of such a randomized algorithm on the worst-case input of size $n$ is defined as: 

$$\max_{I \in I_n} \sum_{A \in A_n} \mathbb{P}_{\sigma_n}[A] \cdot k_n(A, I)$$

### Yao’s Minimax Principle

Yao’s Minimax Principle provides a method for establishing lower bounds on the expected complexity of Las Vegas algorithms. It asserts that the worst-case expected complexity of a randomized algorithm is at least the average-case complexity of the best deterministic algorithm against any fixed probability distribution of inputs.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Yao’s Minimax Principle)</span></p>

Let $\mathcal{A}$ and $\mathcal{I}$ be nonempty finite sets of algorithms and inputs, respectively. Let $k : \mathcal{A} \times \mathcal{I} \to \mathbb{N}$ be a cost function. Let $\sigma$ be a probability distribution on $\mathcal{A}$ and $\tau$ be a probability distribution on $\mathcal{I}$. Let $A_\sigma$ be a random variable with distribution $\sigma$, and $I_\tau$ be a random variable with distribution $\tau$. Then: $\min_{A \in \mathcal{A}}\mathbb{E}[k(A, I_\tau)]\le\max_{I \in \mathcal{I}}\mathbb{E}[k(A_\sigma, I)]$

</div>

**Proof.** To prove this, we define an intermediate value $\tilde{k}$ as the expected cost when both the algorithm and the input are chosen randomly and independently according to $\sigma$ and $\tau$: $\tilde{k} = \sum_{(A, I) \in \mathcal{A} \times \mathcal{I}} \mathbb{P}_\sigma[A] \cdot \mathbb{P}_\tau[I] \cdot k(A, I)$. If $I_\tau$ and $A_\sigma$ are mutually independent, then $\tilde{k} = \mathbb{E}[k(A_\sigma, I_\tau)]$. We now show that $\tilde{k}$ is bounded by the terms in the theorem:

1. First, consider $\tilde{k}$ from the perspective of the distribution $\sigma$: $\tilde{k} = \sum_{A \in \mathcal{A}} \mathbb{P}_\sigma[A] \sum_{I \in \mathcal{I}} \mathbb{P}_\tau[I] \cdot k(A, I) = \sum_{A \in \mathcal{A}} \mathbb{P}_\sigma[A] \cdot \mathbb{E}[k(A, I_\tau)]$. Since the average of a set of values is never less than the minimum value in that set: $\min_{A \in \mathcal{A}} \mathbb{E}[k(A, I_\tau)] \le \tilde{k}$.
2. Next, consider $\tilde{k}$ from the perspective of the distribution $\tau$: $\tilde{k} = \sum_{I \in \mathcal{I}} \mathbb{P}_\tau[I] \sum_{A \in \mathcal{A}} \mathbb{P}_\sigma[A] \cdot k(A, I) = \sum_{I \in \mathcal{I}} \mathbb{P}_\tau[I] \cdot \mathbb{E}[k(A_\sigma, I)]$. Since the average of a set of values is never greater than the maximum value in that set: $\tilde{k} \le \max_{I \in \mathcal{I}} \mathbb{E}[k(A_\sigma, I)]$.

Combining these inequalities, we obtain $\min_{A \in \mathcal{A}} \mathbb{E}[k(A, I_\tau)] \le \tilde{k} \le \max_{I \in \mathcal{I}} \mathbb{E}[k(A_\sigma, I)]$, which completes the proof.

<div class="math-callout math-callout--remark" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Remark</span><span class="math-callout__name"></span></p>

The left-hand side represents the expected cost for the best deterministic algorithm when the input distribution $\tau$ is known. The right-hand side represents the expected worst-case cost for a randomized algorithm defined by $\sigma$.

</div>

### Application: Finding Empty Columns

Consider the problem of determining if an $n \times n$ Boolean matrix has an empty column (a column where all entries are $0$). A black-box algorithm for this problem accesses the matrix only via queries: "Is the entry at row $i$, column $j$ equal to $0$?".

### Upper Bound

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Upper Bound for Empty Columns)</span></p>

There exists a black-box Las Vegas algorithm that decides if a Boolean matrix has an empty column with an expected worst-case cost of: $g_{upper}(n) = \frac{n(n+1)}{2}$

</div>

**Proof.** Consider an algorithm that:

1. Chooses a random permutation $\pi$ of the columns $\lbrace 1, \dots, n\rbrace$.
2. Successively checks columns $\pi(1), \pi(2), \dots$
3. In each column, it checks rows $1, \dots, n$ in order until it finds a 1 or finishes the column. On any input, if an empty column exists, the expected number of checked columns before finding it is at most $(n+1)/2$. Each column takes at most $n$ queries. If no empty column exists, the algorithm will eventually terminate. The expected total queries is at most $n(n+1)/2$.

### Lower Bound

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Lower Bound for Empty Columns)</span></p>

For any black-box Las Vegas algorithm for the empty column problem, the expected worst-case cost is at least: $g_{lower}(n) = \frac{n(n+1)}{2}$

</div>

**Proof.** Using Yao’s Minimax Principle, it suffices to find a distribution $\tau_n$ on inputs $I_n$ such that for any deterministic algorithm $A$, $\mathbb{E}[k(A, I_{\tau_n})] \ge g_{lower}(n)$.

For a given $\epsilon > 0$, define a distribution $\tau_{\epsilon^n}$:

* Let $D_n$ be the set of $n \times n$ matrices with exactly one '1' per column.
* Assign probability $1-\epsilon$ to $D_n$ (distributed uniformly within $D_n$).
* Assign probability $\epsilon$ to $I_n \setminus D_n$ (distributed uniformly).

For an input chosen from $D_n$, a deterministic algorithm can only reject the possibility of an empty column after finding a '1' in every column. In a single column where exactly one entry is '1', the expected number of queries to find that '1' is: $\sum_{i=1}^{n} \frac{i}{n} = \frac{n+1}{2}$ Since there are $n$ columns, the total expected queries for a matrix in $D_n$ is $n \cdot \frac{n+1}{2}$. Multiplying by the probability of choosing from $D_n$, the expected cost is $(1-\epsilon) \frac{n(n+1)}{2}$. As $\epsilon \to 0$, the lower bound approaches $n(n+1)/2$.


## Excursus on Game Theory

The framework of Yao's Minimax Principle is deeply rooted in game theory. A matrix game $(\mathcal{A}, \mathcal{I}, k)$ involves two players:

1. Player I chooses $A \in \mathcal{A}$ to minimize the payoff $k(A, I)$.
2. Player II chooses $I \in \mathcal{I}$ to maximize the payoff $k(A, I)$.

They choose simultaneously. This is a finite two-player zero-sum game with incomplete information.

### Mixed Strategies and Optimality

A pure strategy is a single choice from $\mathcal{A}$ or $\mathcal{I}$. A mixed strategy is a probability distribution ($\sigma$ for Player I, $\tau$ for Player II). The expected payoff for mixed strategies is: $\mathbb{E}[k(A_\sigma, I_\tau)] = \sum_{A, I}\mathbb{P}_\sigma[A]\cdot\mathbb{P}_\tau[I]\cdot k(A, I)$

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Optimal Strategy)</span></p>

$\max_\tau\mathbb{E}[k(A_{\sigma^{\ast}}, I_\tau)] = \min_\sigma\max_\tau\mathbb{E}[k(A_\sigma, I_\tau)]$ A mixed strategy $\tau^{\ast}$ for Player II is optimal if: $\min_\sigma\mathbb{E}[k(A_\sigma, I_{\tau^{\ast}})] = \max_\tau\min_\sigma\mathbb{E}[k(A_\sigma, I_\tau)]$

</div>

In every matrix game, optimal strategies exist because the sets of mixed strategies are compact and payoffs are bounded. Let $k_1^{\ast}$ be the value Player I can enforce (the minimum possible maximum) and $k_2^{\ast}$ be the value Player II can enforce (the maximum possible minimum). It is always true that $k_2^{\ast}\lek_1^{\ast}$.

### Von Neumann’s Minimax Theorem

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Von Neumann’s Minimax Theorem)</span></p>

$\max_\tau \min_\sigma \mathbb{E}[k(A_\sigma, I_\tau)] = \min_\sigma \max_\tau \mathbb{E}[k(A_\sigma, I_\tau)]$

</div>

Variant: 

$$\max_\tau \min_{A \in \mathcal{A}} \mathbb{E}[k(A, I_\tau)] = \min_\sigma \max_{I \in \mathcal{I}} \mathbb{E}[k(A_\sigma, I)]$$

This theorem implies that $k_1^{\ast} = k_2^{\ast}$. The common value is called the value of the game.

### Equilibrium Points

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Equilibrium Point)</span></p>

Equilibrium Point A pair $(\sigma_0, \tau_0)$ is an equilibrium point if neither player can improve their payoff by unilaterally switching strategies:

</div>

1. $\forall \sigma: \mathbb{E}[k(A_{\sigma_0}, I_{\tau_0})] \le \mathbb{E}[k(A_\sigma, I_{\tau_0})]$
2. $\forall \tau: \mathbb{E}[k(A_{\sigma_0}, I_\tau)] \le \mathbb{E}[k(A_{\sigma_0}, I_{\tau_0})]$

If $k_1^{\ast} = k_2^{\ast}$, any pair of optimal strategies ($\sigma^{\ast}, \tau^{\ast}$) forms an equilibrium point. While zero-sum games always have equilibrium points with the same expected payoff, general-sum games (like the ones below) may have multiple equilibria with different payoffs.

### Examples of Non-Zero-Sum Games

1. Planes on Collision Course

Two planes are colliding. Pilots can "Climb" or "Descend." They avoid collision only if they choose different actions. | Plane I \ Plane II | Descend | Climb | | :--- | :--- | :--- | | Descend | (-100, -100) | (0, 0) | | Climb | (0, 0) | (-100, -100) |

There are two pure-strategy equilibria: (Descend, Climb) and (Climb, Descend), both yielding payoff 0. If both pilots toss a fair coin (mixed strategy), the expected payoff is -50.

2. Prisoner’s Dilemma

Two suspects can "Deny" or "Confess." | Suspect I \ Suspect II | Deny | Confess | | :--- | :--- | :--- | | Deny | (1, 1) | (5, 0) | | Confess | (0, 5) | (3, 3) | (Values represent years in prison; players want to minimize these.)

In this game, "Confess" is a dominating strategy—it is the best choice regardless of the other player's action. The equilibrium is (Confess, Confess), resulting in 3 years each. However, if both "Denied," they would only serve 1 year each. This illustrates a conflict between individual rationality and collective benefit.



## Randomized Algorithms: Sorting, Fingerprinting, and Program Self-Correction

In the study of algorithms, randomization serves as a powerful tool to achieve efficiency and robustness. This guide explores the application of randomized techniques to fundamental problems: sorting, file identification through fingerprinting, pattern matching in strings, and the development of self-correcting programs. We focus on Las Vegas algorithms, which always produce a correct result but have a variable running time, and Monte Carlo algorithms, which have a fixed running time but a small probability of error.

### Randomized Sorting

Sorting is a cornerstone of computer science. When considering black-box sorting, we assume the only way to gain information about the input is by comparing two items. The goal is to minimize the expected number of comparisons for a list of $n$ items.

### The RandQuicksort Algorithm

The RandQuicksort algorithm is a randomized version of the classic Quicksort. While deterministic Quicksort can perform poorly on specific "bad" inputs (requiring up to $O(n^2)$ comparisons), RandQuicksort behaves consistently across all possible inputs because it makes random internal choices.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(RandQuicksort)</span></p>

Given a list $S = (s_{\pi(1)}, \dots, s_{\pi(n)})$ of $n$ pairwise distinct items:

</div>

1. Pick an item $s$ from $S$ uniformly at random. This is the pivot.
2. Create two sublists:
  * $S_{small} = (s_{\pi(i)})$ where $s_{\pi(i)} < s$
  * $S_{large} = (s_{\pi(i)})$ where $s_{\pi(i)} > s$
3. If $\lvert S_{small}\rvert > 1$, recursively call RandQuicksort($S_{small}$).
4. If $\lvert S_{large}\rvert > 1$, recursively call RandQuicksort($S_{large}$).
5. Output: The concatenation ($S_{small} \circ s \circ S_{large}$).

### Upper Bound on Complexity

The efficiency of RandQuicksort is measured by the expected number of comparisons.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Upper Bound for RandQuicksort)</span></p>

When RandQuicksort is run on any list of $n$ pairwise distinct items, the expected number of comparisons is at most $1.4n \log n$.

</div>

**Proof.** Let $S = (s_1, \dots, s_n)$ be the sorted version of the input items. In RandQuicksort, any pair of items $(s_i, s_j)$ is compared at most once. This occurs because once a pivot is chosen, it is compared to everything in its current sublist and then never compared again.

1. Let $X_{ij}$ be an indicator variable such that $X_{ij} = 1$ if $s_i$ and $s_j$ are compared, and $X_{ij} = 0$ otherwise, for $1 \le i < j \le n$.
2. The total number of comparisons $X$ is the sum of these indicators: $X = \sum_{1 \le i < j \le n} X_{ij}$.
3. By the linearity of expectation: $\mathbb{E}[X] = \sum_{1 \le i < j \le n}\mathbb{E}[X_{ij}] = \sum_{1 \le i < j \le n}p_{ij}$, where $p_{ij}$ is the probability that $s_i$ and $s_j$ are compared.
4. Consider the set of items $\lbrace s_i, s_{i+1}, \dots, s_j\rbrace$. These items remain in the same sublist until one of them is chosen as a pivot.
5. $s_i$ and $s_j$ are compared if and only if either $s_i$ or $s_j$ is the first item chosen as a pivot from this set of $j - i + 1$ items.
6. Since the pivot is chosen uniformly at random, each item in the set has a probability of $\frac{1}{j-i+1}$ of being chosen first.
7. Thus, $p_{ij} = \frac{2}{j-i+1}$.
8. The expected number of comparisons is: $\mathbb{E}[X] = \sum_{i=1}^{n-1}\sum_{j=i+1}^{n}\frac{2}{j-i+1}$
9. By changing the variable $k = j - i + 1$: $\mathbb{E}[X] = 2 \sum_{i=1}^{n-1}\sum_{k=2}^{n-i+1}\frac{1}{k}\le 2n\sum_{k=2}^{n}\frac{1}{k} = 2n(H_n - 1)$ where $H_n$ is the $n$-th Harmonic number.
10. Using the approximation $H_n - 1 \le \ln n$ and knowing $\ln n = \ln 2 \cdot \log n \approx 0.69 \log n$: $\mathbb{E}[X] \le 2n(0.69 \log n) \approx 1.38 n \log n < 1.4 n \log n$. This concludes the proof.

### Lower Bound for Black-Box Sorting

The upper bound for RandQuicksort is nearly optimal for any black-box Las Vegas algorithm.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Lower Bound for Average Case)</span></p>

Let $A$ be a deterministic black-box sorting algorithm. For any $\epsilon > 0$ and almost all $n$, the average number of comparisons on inputs of size $n$ is at least $(1 - \epsilon)n \log n$.

</div>

**Proof.**

1. Let I be the set of all $N = n!$ permutations of $\lbrace 1, \dots, n\rbrace$.
2. For an algorithm A, let $w_i$ be the sequence of "answer bits" (the results of comparisons) for input $S_i$.
3. The set $\lbrace w_1, \dots, w_N\rbrace$ must be prefix-free. If $w_i$ were a prefix of $w_j$, the algorithm would not be able to distinguish between $S_i$ and $S_j$ after $\lvert w_i\rvert$ comparisons.
4. To minimize the average length $\frac{1}{N} \sum \lvert w_i\rvert$, the word lengths should be as close as possible. In a binary tree with $N$ leaves, the average depth is at least $\log N$.
5. Using Stirling's approximation or basic factorials: $\log n! \ge (1 - \frac{1}{k})n(\log n - \log k)$.
6. By picking $k$ sufficiently large, we find that for large $n$, the average number of comparisons $t$ satisfies $t \ge (1 - \epsilon)n \log n$.

By Yao’s Minimax Principle, the lower bound for the average-case performance of deterministic algorithms under a uniform distribution also serves as a lower bound for the worst-case expected performance of any randomized Las Vegas algorithm. Thus, no randomized sorting algorithm can significantly beat the $n \log n$ bound.


Fingerprinting

Fingerprinting is a technique used to verify the identity of two large files without comparing them bit-by-bit. This is useful for detecting transmission errors or versioning.

### Computing a Fingerprint

We treat a file $w = w_1 \dots w_n$ as a binary string.

1. Map $w$ to an integer $f(w) = \sum_{i=1}^{n} w_i 2^{n-i}$.
2. Choose a small integer $k$ and define the fingerprint as $f_k(w) = f(w) \pmod k$.

Two files $u$ and $v$ have the same fingerprint if $k$ divides $\lvert f(u) - f(v)\rvert$. While a fixed $k$ might fail against specific errors, picking a random prime $p$ provides strong probabilistic guarantees.

### The Prime Number Lemma

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Lemma</span><span class="math-callout__name">(Prime Density)</span></p>

For sufficiently large $t$, there are at least $t$ prime numbers less than or equal to $t \log t$.

</div>

**Proof.** According to the Prime Number Theorem, the number of primes $\pi(m)$ below $m$ is approximately $\frac{m}{\ln m}$. For $m = t \log t$, the count of primes is $\frac{t \log t}{\ln(t \log t)}$. For large $t$, this ratio $\ge t$.

### Algorithm Fingerprint

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Algorithm</span><span class="math-callout__name">(Fingerprint)</span></p>

Fingerprint

</div>

1. Given words $u$, $v$ of length $n$ and parameter $t$.
2. Choose a prime $p$ uniformly at random from the primes below $tn\log(tn)$.
3. If $f_p(u) = f_p(v)$, accept; else reject.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Fingerprint Correctness)</span></p>

If $u = v$, the algorithm accepts with probability $1$. If $u \ne v$, the algorithm accepts with probability at most $1/t$.

</div>

**Proof.**

1. If $u = v$, then $f(u) = f(v)$, so $f(u) \equiv f(v) \pmod p$ for any $p$.
2. If $u \ne v$, let $d = \lvert f(u) - f(v)\rvert$. Since $u$, $v$ are $n$-bit strings, $1 \le d < 2^n$.
3. Any integer $d$ has at most $n$ distinct prime factors (since each prime $\ge 2$).
4. The algorithm picks $p$ from a set of at least $tn$ primes (by the Lemma).
5. The probability that the chosen $p$ is one of the $\le n$ prime factors of $d$ is

   $$\mathbb{P}[\text{False Accept}] \le \frac{n}{tn} = \frac{1}{t}$$

## Pattern Matching

Pattern matching involves finding a pattern $w$ of length $n$ within a longer text a of length $m$.

### Rolling Fingerprints

Instead of bitwise comparisons, we compare the fingerprint of $w$ with the fingerprints of all subwords of $a$ of length $n$. Let $u = a_i \dots a_{i+n-1}$ be a subword and $u' = a_{i+1} \dots a_{i+n}$ be the next subword. The fingerprint $f_p(u')$ can be computed efficiently from $f_p(u)$: $f_p(u') = [2(f_p(u) - 2^{n-1}a_i) + a_{i+n}] \pmod p$ This allows us to slide the "fingerprint window" across the text in $O(m)$ time.

### Error Handling

* Monte Carlo Version: Returns the index as soon as a fingerprint match is found. There is a small risk of a "false match."
* Las Vegas Version: When fingerprints match, the algorithm verifies the match by checking the subwords bit-by-bit. If it's a false match, it continues.

Note on Bad Inputs: For specific pairs of pattern/text (e.g., $w=0^n$, $a=1^m$), certain primes $p$ might cause many false matches. To mitigate this, one can pick a new random prime $p$ whenever a false match is detected.

## Self-Correcting Programs

Even well-tested software or hardware (like the 1994 Pentium division bug) can contain rare errors. Self-correcting programs use randomization to turn a procedure that is occasionally wrong into one that is correct with high probability.

### Error Detection: MultiplicationCheck

To check if $ab = c$ for $n$-bit numbers:

1. Pick a random prime $p$ from the first $tn\log(tn)$ primes.
2. Check if $(a \pmod p \cdot b \pmod p) \pmod p = c \pmod p$. This identifies errors with probability $1 - 1/t$.

### Error Correction: MultiplicationCorrection

Suppose we have a multiplication function that is correct for most, but not all, inputs. We can use the following identity: $ab = (a + r_1)(b + r_2) - r_1(b + r_2) - r_2(a + r_1) + r_1r_2$ where $r_1, r_2$ are random offsets.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Correction Probability)</span></p>

Correction Probability If a processor's multiplication is incorrect for at most a fraction $\epsilon$ of all pairs of $(n+1)$-bit numbers (but addition/subtraction are correct), then MultiplicationCorrection computes ab correctly with probability at least $1 - 16\epsilon$.

</div>

**Proof.**

1. The formula involves four multiplications: $M_1 = (a+r_1)(b+r_2)$, $M_2 = r_1(b+r_2)$, $M_3 = r_2(a+r_1)$, and $M_4 = r_1r_2$.
2. For each multiplication, the arguments are effectively randomized. For example, in $M_2$, $r_1$ and $b+r_2$ are chosen uniformly and independently from the set of $n$-bit (or $n+1$-bit) numbers.
3. Because the inputs are randomized, the probability that any single multiplication $M_i$ hits a "bad" input pair is at most $4\epsilon$.
4. By the sum bound (union bound), the probability that at least one of the four multiplications is incorrect is at most $4 \times 4\epsilon = 16\epsilon$.
5. Thus, the algorithm succeeds with probability at least $1 - 16\epsilon$.

This technique effectively spreads the "bad" inputs across the entire input space, ensuring that for any $a$ and $b$, the probability of failure is consistently low.

## Karger’s Min-Cut Algorithm

The min-cut problem is a fundamental challenge in graph theory, centered on finding the most efficient way to partition a graph. In a deterministic context, minimum cuts can be computed in polynomial time. However, Karger’s algorithm introduces a randomized approach to solving this problem using the concept of edge contraction in multigraphs.

### Foundations of Cuts and Multigraphs

To understand Karger’s algorithm, we must first establish the definitions of cuts and the structures they operate upon.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Min-Cut Problem)</span></p>

A cut of a graph $G = (V, E)$ is a partition of $V$ into two disjoint subsets $V_0$ and $V_1$. The weight of a cut $(V_0, V_1)$ is the number of edges between $V_0$ and $V_1$. A cut is nontrivial if both $V_0$ and $V_1$ are non-empty. A minimum cut of a graph $G$ is a nontrivial cut that has the minimum weight among all possible nontrivial cuts. The size of this cut represents the minimum number of edges that must be removed to disconnect the graph.

</div>

While standard graphs have at most one edge between any two nodes, Karger’s algorithm utilizes multigraphs, which allow for multiple edges between nodes.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Multigraph)</span></p>

A multigraph is a pair $G = (V, E)$ where $V$ is a finite set of nodes and $E : V \times V \to \mathbb{N}$ is a mapping representing edge multiplicities. For all $u, v \in V$, the multigraph must satisfy $E(u, v) = E(v, u)$ (undirected) and $E(u, u) = 0$ (no loops). The multiplicity of an edge $\lbrace u, v\rbrace$ is given by $E(u, v)$.

</div>

In a multigraph, the number of edges $m(E)$, counted with multiplicities, is defined as

$$
m(E) = \frac{1}{2} \sum_{(u,v) \in V \times V} E(u, v) = \sum_{\lbrace u,v\rbrace \in V \times V : u < v} E(u, v).
$$

The concept of a cut extends naturally to multigraphs: the weight $w(V_0, V_1)$ of a cut is the sum of the multiplicities of all edges crossing the partition:

$$
w(V_0, V_1) = \sum_{u \in V_0, v \in V_1} E(u, v).
$$

### Edge Contraction

The core mechanism of Karger’s algorithm is the contraction of nodes. When an edge is contracted, the two endpoints are merged into a single node.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Contracting an Edge)</span></p>

Let $G = (V, E)$ be a multigraph and $\lbrace u, v\rbrace$ be a pair of distinct nodes. The multigraph obtained by contracting the edge $\lbrace u, v\rbrace$ towards $u$ is $G' = (V \setminus \lbrace v\rbrace, E')$. For all $x, y \in V \setminus \lbrace u, v\rbrace$, the new multiplicities are:

* $E'(x, y) = E(x, y)$
* $E'(u, x) = E'(x, u) = E(u, x) + E(v, x)$

</div>

This process collapses nodes $u$ and $v$ into a single node. This can be performed even if the multiplicity of the edge is $0$. Note that the resulting multigraph does not depend on the multiplicity of the edge being contracted.

### The Contract Algorithm

The algorithm iteratively reduces the number of nodes in the graph by picking edges at random and contracting them until only two nodes remain. These two final nodes represent the two sets of the cut.

### Algorithm Contract

1. Input: A graph $G = (V, E)$ with $n = \lvert V \rvert$ nodes.
2. Initialization: Set $V_n = V$, $E_n = E$, and $G_n = (V_n, E_n)$.
3. Iterative Contraction: For $i = n$ down to $3$:
  * Choose an edge $\lbrace u, v\rbrace \in E_i$ uniformly at random based on edge multiplicities. The probability of choosing edge $\lbrace u, v\rbrace$ is $\frac{E_i(u, v)}{m(E_i)}$.
  * Create $G_{i-1} = (V_{i-1}, E_{i-1})$ by contracting nodes u and v into a single node (e.g., $\min\lbrace u, v\rbrace$).
4. Output: The cut ($U_0$, $U_1$) formed by the sets of original nodes that were contracted into the two remaining nodes in $V_2$.

### Success Probability and Analysis

Karger’s algorithm is a Monte Carlo algorithm; it does not guarantee a minimum cut in a single run, but it has a provable lower bound on the probability of success.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Success Probability of Contract)</span></p>

If algorithm Contract is run on a graph with $n$ nodes, any specific minimum cut of the graph is output with probability at least $\frac{2}{n^2}$.

</div>

**Proof.**

1. Let $G$ be a graph with $n$ nodes and fix a specific minimum cut of weight $k$ (the designated cut).
2. Let $G_i = (V_i, E_i)$ be the graphs constructed at each step $i = n, \dots, 2$.
3. For any set of nodes $U$ in $G_i$, let expand($U$) be the set of original nodes in $V$ contracted into $U$. If $(U_0, U_1)$ is a cut in $G_i$, then (expand($U_0$), expand($U_1$)) is a cut in $G$ with the same weight. Thus, every cut in $G_i$ has weight at least $k$.
4. Since every node v in $G_i$ represents a cut $(\lbrace v\rbrace, V_i \setminus \lbrace v\rbrace)$, the degree of every node in $G_i$ must be at least $k$. Therefore, the total number of edges $m(E_i)$ satisfies: $m(E_i) \ge \frac{k \lvert V_i\rvert}{2} = \frac{k \cdot i}{2}$
5. We say $G_i$ is good if no edge crossing the designated cut was contracted during the steps $n, \dots, i$. The algorithm succeeds if $G_2$ is good.
6. If $G_i$ is good, the probability that $G_{i-1}$ is also good is the probability of not picking one of the $k$ edges of the designated cut from the $m(E_i)$ available edges: $\mathbb{P}[G_{i-1} \text{ is good} \mid G_i \text{ is good}] \ge 1 - \frac{k}{m(E_i)} \ge 1 - \frac{k}{ki/2} = 1 - \frac{2}{i} = \frac{i-2}{i}$
7. By the chain rule, the probability that $G_2$ is good is:

   $$
   \mathbb{P}[G_2 \text{ is good}] = \prod_{i=n}^{3} \mathbb{P}[G_{i-1} \text{ is good} \mid G_i \text{ is good}] \ge \prod_{i=n}^{3} \frac{i-2}{i}
   $$

   $$
   = \left(\frac{n-2}{n}\right)\left(\frac{n-3}{n-1}\right)\cdots\left(\frac{2}{4}\right)\left(\frac{1}{3}\right) = \frac{2}{n(n-1)} > \frac{2}{n^2}
   $$

   This concludes the proof.

To increase the reliability of finding the minimum cut, the algorithm is typically run multiple times independently.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Corollary</span><span class="math-callout__name">(Probability Amplification)</span></p>

Assume algorithm Contract is run independently $tn^2$ times on a graph with $n$ nodes. The probability that the minimum cut is not found is at most $\frac{1}{2^{2t}}$.

</div>

**Proof.** Using the result from the theorem, the probability of failure in one run is at most $(1 - \frac{2}{n^2})$. For $tn^2$ runs:

$\mathbb{P}[\text{Failure}] \le \left(1 - \frac{2}{n^2}\right)^{tn^2} = \left[\left(1 - \frac{2}{n^2}\right)^{n^2/2}\right]^{2t} \le \left(\frac{1}{e}\right)^{2t} < \frac{1}{2^{2t}}$

The inequality $(1 - \frac{1}{x})^x \le \frac{1}{e}$ is used here.

## Fundamental Probabilistic Inequalities

Probabilistic inequalities (or tail bounds) allow us to bound the probability that a random variable deviates significantly from its expected value.

### Markov’s Inequality

Markov’s inequality is the most basic tail bound, requiring only that the random variable be non-negative and have a known expectation.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Markov’s Inequality)</span></p>

Let X be a discrete random variable that attains only non-negative values such that $\mathbb{E}[X]$ exists. For any real number $r > 0$: $\mathbb{P}[X \ge r] \le \frac{\mathbb{E}[X]}{r}$ If $\mathbb{E}[X] > 0$, this can be rewritten as $\mathbb{P}[X \ge r \mathbb{E}[X]] \le \frac{1}{r}$.

</div>

**Proof.** Let $X_r$ be an indicator variable where $X_r = 1$ if $X \ge r$ and $X_r = 0$ otherwise. Since $X$ is non-negative, $X_r \le \frac{X}{r}$. Taking the expectation of both sides: $\mathbb{P}[X \ge r] = \mathbb{E}[X_r] \le \mathbb{E}\left[\frac{X}{r}\right] = \frac{\mathbb{E}[X]}{r}$

### Chebyshev’s Inequality

Chebyshev's inequality provides a tighter bound by utilizing the variance of the random variable.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Variance)</span></p>

The variance $\mathrm{Var}[X]$ of a discrete random variable $X$ is defined as

$$\mathrm{Var}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2]$$

Small variance indicates that $X$ is highly concentrated around its mean $\mathbb{E}[X]$. The square root of the variance is a rough measure of deviation from the mean.

</div>

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Chebyshev’s Inequality)</span></p>

Let $X$ be a random variable where $\mathbb{E}[X]$ and $\mathrm{Var}[X]$ exist. For any $r > 0$: $\mathbb{P}[\lvert X - \mathbb{E}[X]\rvert \ge r]\le\frac{\mathrm{Var}[X]}{r^2}$ If $\mathrm{Var}[X] > 0$, then $\mathbb{P}[\lvert X - \mathbb{E}[X]\rvert \ge r \sqrt{\mathrm{Var}[X]}]\le\frac{1}{r^2}$.

</div>

**Proof.** Apply Markov's inequality to the non-negative random variable $Y = \lvert X - \mathbb{E}[X]\rvert^2$. Note that $\mathbb{E}[Y] = \mathrm{Var}[X]$. $\mathbb{P}[\lvert X - \mathbb{E}[X]\rvert \ge r\sqrt{\mathrm{Var}[X]}] = \mathbb{P}[\lvert X - \mathbb{E}[X]\rvert^2 \ge r^2 \mathrm{Var}[X]]\le\frac{\mathrm{Var}[X]}{r^2 \mathrm{Var}[X]} = \frac{1}{r^2}$

### The Chernov-Hoeffding Bound

The Chernov-Hoeffding bound provides exponentially decreasing bounds for the sum of independent $\lbrace 0, 1\rbrace$-valued random variables.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Chernov-Hoeffding Bound)</span></p>

Chernov-Hoeffding Bound Let $X_1, \dots, X_n$ be mutually independent $\lbrace 0, 1\rbrace$-valued random variables where $\mathbb{P}[X_i = 1] = p_i$. Let $X = \sum X_i$ and $\mu = \mathbb{E}[X] = \sum p_i$.

</div>

1. Lower Tail: For $0 < \delta \le 1: \mathbb{P}[X \le (1 - \delta)\mu] < e^{-\frac{\delta^2 \mu}{2}}$
2. Upper Tail: For $\delta > 0: \mathbb{P}[X \ge (1 + \delta)\mu] < \left(\frac{e^\delta}{(1 + \delta)^{(1 + \delta)}}\right)^\mu$

### Proof of Lower Tail

1. Using the fact that the exponential function is strictly increasing, for any $t > 0$: $\mathbb{P}[X \le (1 - \delta)\mu] = \mathbb{P}[e^{-tX} \ge e^{-t(1-\delta)\mu}]\le \frac{\mathbb{E}[e^{-tX}]}{e^{-t(1-\delta)\mu}}$
2. Since $X_i$ are independent, $\mathbb{E}[e^{-tX}] = \prod_{i=1}^n$ \mathbb{E}[e^{-tX_i}]$.
3. $\mathbb{E}[e^{-tX_i}] = p_i e^{-t} + (1 - p_i) = 1 + p_i(e^{-t} - 1) \le e^{p_i(e^{-t} - 1)}$ (using $1+y < e^y$).
4. Thus, $\mathbb{E}[e^{-tX}]\le e^{(e^{-t}-1)\mu}$.
5. Let $b(t) = e^{-t} - 1 + t(1-\delta)$. We want to minimize $e^{b(t)\mu}$. The minimum of $b(t)$ occurs at $t_0 = \ln(1/(1-\delta))$.
6. Plugging $t_0$ into the bound: $\mathbb{P}[X \le (1-\delta)\mu]\le\left(\frac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^\mu$
7. Using the Taylor expansion $\ln(1-\delta) = -\delta - \frac{\delta^2}{2} - \dots$, we find $(1-\delta)\ln(1-\delta) > -\delta + \frac{\delta^2}{2}$.
8. This leads to the final bound $e^{(-\delta + \delta - \delta^2/2)\mu} = e^{-\delta^2\mu/2}$.

### Proof of Upper Tail

1. Similar to the lower tail, for $t > 0$: $\mathbb{P}[X \ge (1+\delta)\mu] \le \frac{\mathbb{E}[e^{tX}]}{e^{t(1+\delta)\mu}}$.
2. $\mathbb{E}[e^{tX}] < e^{(e^t - 1)\mu}$.
3. Minimizing the exponent $b(t) = e^t - 1 - t(1+\delta)$ gives $t_0 = \ln(1+\delta)$.
4. Substituting $t_0$ yields the result: $\left(\frac{e^\delta}{(1+\delta)^{(1+\delta)}}\right)^\mu$.

## Application: Majority Vote and Probability Amplification

A common application of tail bounds is probability amplification through a majority vote. If a randomized algorithm correctly decides a property with probability $p = 1/2 + \epsilon$, we can repeat the algorithm n times and take the majority output to significantly lower the error probability.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Corollary</span><span class="math-callout__name">(Probability Amplification)</span></p>

Let $X_1, \dots, X_n$ be independent $\lbrace 0, 1\rbrace$ variables where $\mathbb{P}[X_i = 1] = 1/2 + \epsilon$ (correctness). Let $X = \sum X_i. The probability that the majority vote is wrong (i.e., $X \le n/2$) is: $\mathbb{P}[X \le n/2] < 2^{-\epsilon^2 n}$

</div>

**Proof.** Let $\mu = \mathbb{E}[X] = (1/2 + \epsilon)n$. We look for $\delta$ such that $n/2 \le (1-\delta)\mu. Solving $n/2 = (1-\delta)(1/2 + \epsilon)n$ gives $\delta = \frac{2\epsilon}{1 + 2\epsilon}$. Applying the Chernov bound: $\mathbb{P}[X \le n/2] \le e^{-\frac{\delta^2 \mu}{2}} = e^{-\frac{\epsilon^2 n}{1+2\epsilon}} < e^{-\epsilon^2 n} < 2^{-\epsilon^2 n}$

**Note.** To achieve an error probability of at most $2^{-t}$, one should set the number of iterations $n \ge \frac{2}{\epsilon^2} t$.


## Randomized Local Search for Boolean Satisfiability

This guide explores the application of randomized local search techniques to the problem of finding satisfying assignments for Boolean formulas, specifically focusing on formulas in 3-conjunctive normal form (3-CNF). By modeling the search process as a one-dimensional random walk, we can derive the success probabilities and time complexity of these algorithms.

### Fundamental Concepts of Boolean Formulas

To understand the search algorithm, we must first define the structure of the input formulas.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Literal)</span></p>

A literal is a Boolean variable ($Z_i$) or its negation ($\neg Z_i$).

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Clause)</span></p>

A (disjunctive) clause is a disjunction (an OR operation) of literals.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Conjunctive Normal Form)</span></p>

A Boolean formula is in conjunctive normal form (CNF) if it is a conjunction (an AND operation) of clauses.

</div>

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(k-Conjunctive Normal Form)</span></p>

A Boolean formula is in $k$-CNF if it is in CNF such that each clause contains exactly $k$ mutually distinct variables. (Note: Some liberal definitions allow at most $k$ literals).

</div>

### Satisfying Assignments

An assignment of length $n$ is a binary word $\sigma \in \lbrace 0, 1\rbrace^n$, where 0 represents false and 1 represents true. For a formula $\phi$ over $n$ variables, we assume the variables are ordered, meaning the $i$-th bit of the assignment corresponds to the $i$-th variable.

A formula is satisfied if the assignment makes the entire formula evaluate to true. For example, consider the 3-CNF formula: $\phi = (Z_1 \lor \neg Z_3 \lor Z_4) \land (\neg Z_1 \lor Z_4 \lor Z_5)$. The assignment 10010 satisfies this formula, while 10000 does not.

### Hamming Distance

To measure how "close" an assignment is to a solution, we use the Hamming distance.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Hamming Distance)</span></p>

The distance $d(\sigma, \tau)$ between two words $\sigma$ and $\tau$ of the same length is the number of positions where the words differ.

</div>

### The Logic of Local Search

Consider a 3-CNF formula $\phi$ that is satisfied by an assignment $\sigma_0$. If we have a current assignment $\sigma$ that does not satisfy $\phi$, there must be at least one clause in $\phi$ that evaluates to false under $\sigma$.

Because $\sigma_0$ satisfies every clause, it must differ from the current (unsatisfying) assignment $\sigma$ on at least one of the variables present in that unsatisfied clause. In a 3-CNF formula, each clause has $3$ variables. Therefore, if we pick an unsatisfied clause and flip the value of one of its variables at random:

* With probability at least $1/3$, we flip a variable where $\sigma$ and $\sigma_0$ differ, thereby decreasing the Hamming distance $d(\sigma, \sigma_0)$ by 1.
* With probability at most $2/3$, we might flip a variable where $\sigma$ and $\sigma_0$ were already identical, increasing the distance by 1.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Algorithm</span><span class="math-callout__name">(LocalSearch)</span></p>

LocalSearch

</div>

The following algorithm utilizes this "flipping" strategy to navigate the search space.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Algorithm</span><span class="math-callout__name">(LocalSearch($\phi$, $\sigma$, $r$))</span></p>


</div>

Input: A Boolean formula $\phi$ in n variables in 3-CNF, an assignment $\sigma \in \lbrace 0, 1\rbrace^n$, and a natural number $r$.

1. While $r \ge 1$ and $\phi(\sigma) = \text{false}$:
  * Pick the least clause of $\phi$ that is not made true by $\sigma$.
  * Pick a variable in this clause uniformly at random.
  * Flip the value of the assignment $\sigma$ at this variable.
  * Let $r = r - 1$.
2. Output: The modified assignment $\sigma$.

### Modeling Search as a Random Walk

The progress of the LocalSearch algorithm can be modeled as a one-dimensional random walk. If the initial assignment $\sigma$ is at distance $j$ from a satisfying assignment $\sigma_0$, each step of the loop moves the "token" (the current distance) either left (closer to $\sigma_0$) with probability $\alpha \ge 1/3$ or right (further from $\sigma_0$) with probability $1 - \alpha \le 2/3$.

Reaching a satisfying assignment is equivalent to the random walk reaching the origin (position 0).

### Excursus: The Probability of Reaching the Origin

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(One-Dimensional Random Walk)</span></p>

A sequence of random variables $X_0, X_1, \dots$ where:

</div>

* If $X_t = 0$, then $X_{t+1} = 0$ (the origin is an absorbing point).
* If $X_t \neq 0$, $X_{t+1}$ is $X_t - 1$ with probability $\alpha$ and $X_t + 1$ with probability $1 - \alpha$. This sequence is a Markov chain because the distribution of $X_{t+1}$ depends only on the current state $X_t$.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Probability of Reaching the Origin)</span></p>

Let $p_j$ be the probability that a walk starting at $X_0 = j$ eventually reaches $0$.

</div>

1. If $\alpha \ge 1/2$, $p_j = 1$ for all $j \ge 0$.
2. If $\alpha < 1/2$, $p_j = \left(\frac{\alpha}{1-\alpha}\right)^j$.

**Proof.**

1. Induction on $j$: To reach 0 from $j$, the token must first reach $j-1$ from $j$, and then reach 0 from $j-1$. These are independent events with the same underlying dynamics. Thus, $p_j = p_1 \cdot p_{j-1}$, which implies $p_j = p_1^j$.
2. Determining $p_1$: For $j \ge 1$, we have the recurrence: $p_j = \alpha p_{j-1} + (1 - \alpha) p_{j+1}$. For $j = 1$: $p_1 = \alpha p_0 + (1 - \alpha)p_2 = \alpha(1) + (1 - \alpha)p_1^2$. This simplifies to the quadratic equation: $(1 - \alpha)p_1^2 - p_1 + \alpha = 0 \implies (p_1 - 1)\left(p_1 - \frac{\alpha}{1 - \alpha}\right) = 0$
3. Evaluating Cases:
  * If $\alpha\ge 1/2$, then $\frac{\alpha}{1-\alpha}\ge 1$. Since $p_1$ cannot exceed 1, the only solution is $p_1 = 1$.
  * If $\alpha < 1/2$ (the case for 3-SAT where $\alpha = 1/3$), it can be shown via Chernov-Hoeffding bounds that $p_j < 1$. Thus, $p_1 = \frac{\alpha}{1-\alpha}$. For $\alpha = 1/3$: $p_1 = \frac{1/3}{1 - 1/3} = \frac{1/3}{2/3} = \frac{1}{2}$ Therefore, $p_j = (1/2)^j$.

### Restricting Walk Length

The previous calculation assumes an infinite number of moves. For LocalSearch, we limit the steps to $r$.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Lemma</span><span class="math-callout__name">(Success with Finite Moves)</span></p>

Success with Finite Moves For $\alpha = 1/3$ and $r = 27j$, the probability of reaching the origin within $r$ moves is at least $p_j / 2 = 2^{-(j+1)}$.

</div>

**Proof.** By partitioning the event of eventually reaching the origin into reaching it within $27j$ moves versus reaching it later, and applying Chernov-Hoeffding bounds to the latter, it can be demonstrated that the probability of reaching the origin after $27j$ moves is small (less than half of $p_j$). Thus, the majority of the success probability is concentrated within the first $27j$ steps.

### Success Probability of a Single Invocation

To find the total success probability, we must account for the fact that the initial assignment $\sigma$ is chosen uniformly at random.

1. Probability of starting at distance $j$: There are $\binom{n}{j}$ assignments at distance $j$ from a fixed satisfying assignment $\sigma_0$. The probability of picking one is: $\mathbb{P}[d(\sigma, \sigma_0) = j] = \frac{\binom{n}{j}}{2^n}$
2. Total Success Probability ($P_{success}$): By combining the probability of starting at distance $j$ with the probability of succeeding from that distance ($p_j/2 = 2^{-(j+1)}$): $P_{success} \ge \sum_{j=0}^{n} \mathbb{P}[d(\sigma, \sigma_0) = j] \frac{p_j}{2} = \sum_{j=0}^{n} \binom{n}{j} \frac{1}{2^n} \frac{1}{2^{j+1}}$. Factoring out constants: $P_{success} \ge \frac{1}{2^{n+1}} \sum_{j=0}^{n} \binom{n}{j} \left(\frac{1}{2}\right)^j$. Using the Binomial Theorem $(1+x)^n = \sum \binom{n}{j} x^j$ with $x = 1/2: P_{success} \ge \frac{1}{2^{n+1}} \left(1 + \frac{1}{2}\right)^n = \frac{1}{2} \cdot \frac{(3/2)^n}{2^n} = \frac{1}{2}\left(\frac{3}{4}\right)^n$

### Iterated Local Search

Since the success probability of a single run is low ($\frac{1}{2} (3/4)^n$), we repeat the process independently to boost the probability of finding a solution.

**Note.** Independent Trials If an experiment with success probability $1/N$ is repeated $N$ times, the probability of at least one success is $1 - (1 - 1/N)^N \approx 1 - 1/e \approx 0.63$.

By running the algorithm $2(4/3)^n$ times, we achieve a high constant probability of success.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Algorithm</span><span class="math-callout__name">(IteratedLocalSearch($\phi$))</span></p>

IteratedLocalSearch($\phi$)

</div>

Input: A Boolean formula $\phi$ in n variables in 3-CNF.

1. Let $i = 1$ and $\sigma = 0^n$.
2. While $i \le 2 \cdot (4/3)^n$ and $\phi(\sigma) = \text{false}$:
  * Pick an assignment $\sigma$ uniformly at random from $\lbrace 0, 1\rbrace^n$.
  * Let $\sigma$ := LocalSearch($\phi$, $\sigma$, $27n$).
  * Let $i := i + 1$.
3. Output: The current assignment $\sigma$.

### Performance and Complexity

If the formula is satisfiable, this algorithm returns a satisfying assignment with probability at least $0.6$. If the formula is unsatisfiable, it returns a non-satisfying assignment with probability $1$.

The running time is proportional to the number of iterations times the cost of LocalSearch. The complexity is: $O\left(\left(\frac{4}{3}\right)^n \cdot poly(n)\right) \approx O(1.34^n)$. This is significantly better than the $O(2^n)$ required for exhaustive search.


## Exhaustive Local Search and Cryptographic Applications

### Exhaustive Local Search for k-SAT

Exhaustive local search is a systematic approach used to find a satisfying assignment for a Boolean formula by exploring assignments within a specific distance from a starting point. This method is particularly useful for formulas in $k$-CNF (Conjunctive Normal Form where each clause contains exactly $k$ literals).

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Algorithm</span><span class="math-callout__name">(LocalSearchExh($\phi$, $\sigma$, $r$))</span></p>

LocalSearchExh($\phi$, $\sigma$, $r$)

</div>

Input: Natural numbers $n$ and $k \ge 3$, a Boolean formula $\phi$ in $k$-CNF with n variables, an assignment $\sigma \in \lbrace 0, 1\rbrace^n$, and a search radius (depth) $r$.

Procedure:

1. If $\phi(\sigma)$ is true, return $\sigma$ (via a global variable $\alpha$).
2. If $r \ge 1$ and a satisfying assignment has not yet been found:
  * Pick the first (least) clause in $\phi$ that is not satisfied by $\sigma$.
  * For each of the $k$ variables in this clause:
    * Create a new assignment $\sigma$' by flipping the value of that variable in $\sigma$.
    * Recursively invoke LocalSearchExh($\phi$, $\sigma$', $r - 1$).

The algorithm explores a search tree where the root is the initial assignment $\sigma$, and each internal node has $k$ descendants, representing the $k$ possible ways to satisfy the current unsatisfied clause by changing exactly one variable's value.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Verification of LocalSearchExh)</span></p>

Verification of LocalSearchExh

</div>

Let $\phi$ be a Boolean formula in n variables in $k$-CNF. Let $\sigma$ and $\sigma_0$ be any assignments such that the Hamming distance $d(\sigma, \sigma_0) \le r$, where $\sigma_0$ is a satisfying assignment. Invoking LocalSearchExh($\phi$, $\sigma$, $r$) will yield a satisfying assignment.

**Proof.**

* Termination: All recursive calls eventually terminate because the parameter $r$ is decremented at each step. The condition for picking a clause ensures that a search only occurs if the current assignment does not satisfy $\phi$.
* Success: If the global variable $\alpha$ is ever set to a satisfying assignment, it remains unchanged. We prove the proposition by induction on $r$.
  * Base Case ($r = 0$): Since $d(\sigma, \sigma_0) \le 0$, it must be that $\sigma = \sigma_0$. Thus, $\phi(\sigma)$ is true, and $\alpha$ is set to $\sigma$.
  * Inductive Step ($r \ge 1$): If $\phi(\sigma)$ is true, we are done. If not, consider the satisfying assignment $\sigma_0$. Since $\sigma$ fails a clause $C$, and $\sigma_0$ must satisfy $C$, there is at least one variable in $C$ where $\sigma$ and $\sigma_0$ differ. By flipping this variable to create $\sigma$', the distance $d(\sigma', \sigma_0)$ becomes $d(\sigma, \sigma_0) - 1 \le r - 1$. By the inductive hypothesis, the call LocalSearchExh($\phi$, $\sigma$', $r - 1$) will find a satisfying assignment.

### Deterministic 3-SAT Application

A basic deterministic algorithm for 3-SAT can be constructed using exhaustive local search. Any assignment in $\lbrace 0, 1\rbrace^n$ is at most distance $n/2$ from either the all-zeros assignment $0^n$ or the all-ones assignment $1^n$. Therefore, $\phi$ is satisfiable if and only if LocalSearchExh($\phi$, $0^n$, $n/2$) or LocalSearchExh($\phi$, $1^n$, $n/2$) returns an assignment.

The complexity of this approach is $\text{poly}(n) \cdot 3^{n/2}$, which simplifies to $\text{poly}(n) \cdot (\sqrt{3})^n \approx \text{poly}(n) \cdot 1.74^n$. This is significantly faster than the $2^n$ required for a naive search of all possible assignments.

### Mathematical Foundations: Discrete Balls and Entropy

To optimize exhaustive local search, we must understand the "volume" of the search space covered by a specific radius.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Volume of a Discrete Ball)</span></p>

Volume of a Discrete Ball

</div>

The $n$-dimensional ball with center $\sigma \in \lbrace 0, 1\rbrace^n$ and radius $r$ is defined as: $B(\sigma, n, r) = \lbrace \tau \in \lbrace 0, 1\rbrace^n : d(\sigma, \tau) \le r \rbrace$. The volume, denoted $\text{vol}(n, r)$, is independent of the center $\sigma$ and is calculated as: $\text{vol}(n, r) = \sum_{j=0}^{r} \binom{n}{j}$

For large $n$, we approximate this volume using the binary entropy function.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Binary Entropy Function)</span></p>

The binary entropy function $H : (0, 1) \to [0, 1]$ is defined as: 

$$H(\rho) = -\rho \log \rho - (1 - \rho) \log (1 - \rho)$$

</div>


<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Approximate Volume of a Ball)</span></p>

Approximate Volume of a Ball

</div>

Let $\rho = r/n$ and $v(n, r) = 2^{H(\rho)n}$. For sufficiently large $n$ and $r \le n/2: \frac{v(n, r)}{n} \le \text{vol}(n, r) \le v(n, r)$

### Optimizing the Search Radius

An invocation of LocalSearchExh($\phi$, $\sigma$, $r$) checks if any satisfying assignment exists within $B(\sigma, n, r)$. The algorithm explores a search tree with a size bounded by: $a_k(n, r) := \sum_{i=0}^{r} k^i \approx k^r$

The efficiency gain of local search over naive exhaustive search within the ball $B(\sigma, n, r)$ occurs when $a_k(n, r) < \text{vol}(n, r)$. To find the optimal radius, we minimize the ratio of the search tree size to the ball volume. Let $\rho = r/n$. We approximate the ratio as: $\frac{a_k(n, r)}{\text{vol}(n, r)}\le \frac{k^{rn}}{2^{H(\rho)n} / n} = n \cdot 2^{(\rho \log k - H(\rho))n}$

To minimize this, we take the derivative of the exponent with respect to $\rho$. The derivative of $H(\rho)$ is $-\log(\frac{\rho}{1-\rho})$. Setting the derivative of the exponent to zero: $\log k + \log\left(\frac{\rho}{1-\rho}\right) = 0 \iff \frac{\rho}{1-\rho} = \frac{1}{k} \iff \rho = \frac{1}{k+1}$

### Benefits of the Optimum Radius

By setting $r = \frac{n}{k+1}$, the ratio becomes: $\frac{a_k(n, r)}{v(n, r) / n} = n \cdot \left(\frac{k}{k+1}\right)^n$ This indicates that the search tree is exponentially smaller than the total number of assignments in the ball.

### Randomized and Deterministic Search Strategies

### Randomized Local Search

If we choose an initial assignment $\sigma$ uniformly at random, the probability that a specific satisfying assignment $\sigma_0$ falls within $B(\sigma, n, r)$ is $\frac{\text{vol}(n, r)}{2^n}$.

A randomized algorithm can be defined by repeating the local search $t \cdot \frac{2^n}{\text{vol}(n, r)}$ times with independent random starting assignments. The probability of failing to find a satisfying assignment is at most $(1 - \frac{\text{vol}(n, r)}{2^n})^{t \cdot 2^n / \text{vol}(n, r)} \le e^{-t}$.

For $r = \frac{n}{k+1}$ and $t = n^2$, the total number of assignments checked is approximately: $\text{poly}(n) \cdot \left(\frac{2k}{k+1}\right)^n$ For $3$-SAT ($k=3$), this results in a randomized algorithm with a running time of $\text{poly}(n) \cdot 1.5^n$ and an error probability of at most $2^{-n^2}$.

### Derandomization via Covering Codes

To make this algorithm deterministic, we replace random sampling with a covering code.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Covering Code)</span></p>

Covering Code

</div>

A set $C \subseteq \lbrace 0, 1\rbrace^n$ is a covering code of radius $r$ if every word in $\lbrace 0, 1\rbrace^n$ is contained in at least one ball of radius $r$ centered at a codeword in $C$: $\lbrace 0, 1\rbrace^n = \bigcup_{w \in C} B(w, n, r)$

By running LocalSearchExh starting from every codeword in a covering code of radius $r = \frac{n}{k+1}$, we can deterministically decide satisfiability. The size of such a code is roughly $\frac{2^n}{\text{vol}(n, r)}$. The resulting deterministic complexity is $\text{poly}(n) \cdot (\frac{2k}{k+1})^n$.

### Excursus: The Set Cover Problem and Code Construction

The construction of efficient covering codes is related to the Minimum Set Cover Problem.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Weighted Minimum Set Cover Problem)</span></p>

</div>

Given a finite set $A$ and subsets $S_1$, $\dots$, $S_m$ with weights $w_j$, find a subset of indices $J$ such that $\bigcup_{j \in J} S_j = A$ and $\sum_{j \in J} w_j$ is minimized.

The greedy algorithm for set cover provides a solution within a factor of ($1 + \ln n$) of the optimum.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Existence of Small Covering Codes)</span></p>

Existence of Small Covering Codes

</div>

For any $n$ and $r < n$, there exists a covering code $C$ of size: $\lvert C\rvert \le \frac{n \cdot 2^n}{\text{vol}(n, r)} \le \frac{n^2 \cdot 2^n}{v(n, r)}$

**Proof.**

* Suppose we pick a set $C$ of size $\frac{n \cdot 2^n}{vol(n, r)}$ uniformly at random.
* Let $E_u$ be the event that a specific word $u$ is not covered by any ball around words in $C$.
* $\mathbb{P}[E_u] = (1 - \frac{vol(n, r)}{2^n})^{\lvert C\rvert} \le$ e^{-n}$.
* The sum of these probabilities over all $2^n$ words is $2^n \cdot e^{-n}$, which is less than 1 for sufficiently large $n$. Thus, a covering code of this size must exist.

To compute these codes efficiently, we can use a direct sum of smaller covering codes. By partitioning the n-bit word into d blocks of length $n_0 \approx n/d$ and computing optimal codes for these smaller blocks, we can construct a total covering code in $\text{poly}(n)\lvert C\rvert$ time.


## Cryptography: Symmetric and Public-Key Systems

Cryptography traditionally relied on symmetric-key protocols, where parties share a secret key in advance. An example is the One-time pad, where a message $w$ is encrypted using a random secret $r$ of the same length via $w \oplus r$. The receiver recovers $w$ by computing $(w \oplus r) \oplus r$.

### Public-Key Cryptography

In public-key protocols, each party has a private key (kept secret) and a public key (published). These systems rely on a hardness assumption: the belief that certain mathematical problems are computationally infeasible to solve, even with randomized algorithms.

#### Key tasks used in these assumptions include

1. Factorization: Finding prime factors of large integers.
2. Discrete Logarithm Problem: Given a generator $d$ of a cyclic group $G$ and an element $b$, finding $x$ such that $d^x = b$.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Cyclic Group)</span></p>

</div>

A finite group $(G, \circ)$ is cyclic if there exists a generator $d \in G$ such that $G = \lbrace d^1, d^2, \dots, d^{\lvert G\rvert}\rbrace$.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Discrete Logarithm)</span></p>

Discrete Logarithm

</div>

Given a generator $d$ of a cyclic group $G$ of order $n$ and $b \in G$, the discrete logarithm $\log_d b$ is the unique $x \in \lbrace 0, \dots, n-1\rbrace$ such that $b = d^x$.

### Standard Protocols

1. Diffie-Hellman Key Agreement:

* Initialization: A and B publish a prime $p$ and a generator $d$ of $\mathbb{Z}_p^{\ast}$.
* Step 1: A chooses secret $i_A$, B chooses secret $i_B$.
* Step 2: A sends $d^{i_A}$ to B; B sends $d^{i_B}$ to $A$.
* Result: Both compute the shared secret $K = d^{i_A \cdot i_B}$.

2. ElGamal Public-Key Encryption:

* Goal: $B$ wants to send message $m \in\lbrace 1, \dots, p-1\rbrace$ to $A$.
* Infrastructure: $A$ publishes ($p$, $d$, $d^{i_A}$) and keeps $i_A$ secret.
* Encryption: $B$ chooses random $i_B$, computes $c_1 = d^{i_B}$ and $c_2 = m \cdot (d^{i_A})^{i_B}$, and sends ($c_1$, $c_2$) to $A$.
* Decryption: $A$ uses $i_A$ to compute $m = c_2 \cdot (c_1^{i_A})^{-1}$ (using Fermat's Little Theorem: $c_1^{p-1-i_A}$ in $\mathbb{Z}_p^{\ast}$).

**Note.** These protocols are secure only if computing the discrete logarithm is hard. If an adversary could efficiently compute $i_A$ from $d^{i_A}$, they could decrypt any message or intercept any shared key.



## Zero-Knowledge Protocols

Zero-knowledge protocols address a fundamental problem in digital communication: identity verification. In a typical scenario, party A (the Prover) needs to identify herself to party B (the Verifier). This is commonly seen when a user logs onto a computer or a customer accesses an internet bank.

Standard solutions often involve A identifying herself via a secret, such as a password or PIN. To prevent eavesdroppers from learning this secret, protocols may use one-time passwords or tasks that utilize the secret without revealing it directly. The ideal solution is a Zero-knowledge protocol, where even if B deviates from the protocol, B cannot obtain any relevant information about A's secret.

### The k-Coloring Problem

To implement such a protocol, we rely on a hard computational problem. In this context, we use the k-coloring of a graph.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(k-Coloring)</span></p>

Let $G = (V, E)$ be a graph. A $k$-coloring of $G$ is a mapping $g : V \to \lbrace 1, \dots, k\rbrace$. A coloring is considered legal if for all edges $\lbrace u, v\rbrace \in E$, the colors $g(u)$ and $g(v)$ are distinct.

</div>

The protocol assumes that $A$ knows a secret legal $k$-coloring for a graph $G$, and it is computationally infeasible for $B$ to compute such a coloring knowing only $G$ and $k$. This requires a randomized procedure to generate a sufficiently large graph $G$ and its corresponding coloring.

### Protocol Coloring

This protocol allows $A$ to prove she knows the coloring without revealing it.

Assumptions: $A$ knows a legal $k$-coloring $g$ of $G$ and has a private random source.

1. Step 1 (A): A picks a random permutation $\pi$ of the color set $\lbrace 1, \dots, k\rbrace$. A then secretly commits to a list of the permuted colors for all nodes: $\pi(g(1)), \dots, \pi(g(n))$.
2. Step 2 (B): B picks an edge $\lbrace u, v\rbrace$ from $G$ uniformly at random and sends these vertices to $A$.
3. Step 3 (A): A reveals the specific colors $\pi(g(u))$ and $\pi(g(v))$ to $B$.
4. Step 4 (B): B accepts if the two colors are distinct and belong to the set $\lbrace 1, \dots, k\rbrace$. Otherwise, $B$ rejects.

To minimize the probability of $B$ accepting a false claim, the protocol is iterated $m$ times, where $m$ is the number of edges in $G$.

### Analysis of Protocol Goals

The Protocol Coloring is designed to achieve three specific goals:

1. Completeness: A can always successfully verify her identity if she knows the coloring. This is verified by inspection of the protocol steps.
2. Soundness: If A does not have a legal $k$-coloring, the probability that B accepts throughout $m$ iterations is low. If the committed colors do not form a legal coloring, there is at least one edge where the colors are identical. The probability of error after $m$ iterations is: $\left(1 - \frac{1}{m}\right)^m \le \frac{1}{e} \le \frac{1}{2}$
3. Zero-Knowledge: B learns nothing. In each step, B only sees a pair of distinct colors. Because of the random permutation $\pi$, these pairs are mutually independent and uniformly distributed. B could have generated such a distribution of color pairs himself, meaning no new information about the actual coloring $g$ is leaked.

### Mechanisms for Secret Commitment

For the protocol to work, $A$ must "commit" to the colors so she cannot change them after $B$ picks an edge.

* Physical Analogy: $A$ places colored tokens in opaque containers. $A$ cannot change them once placed but can open specific containers for $B$.
* Electronic Implementation: $A$ commits to the bits describing the color sequence. This is done using a one-way function $f$ that is easy to compute, but where it is infeasible to determine specific properties (like parity) of the input $n$ from the output $f(n)$. This often relies on the assumed hardness of problems like the discrete logarithm.


## Sum-Free Subsets

In additive combinatorics, we explore subsets of integers where no two elements sum to a third element within the same set.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Sum-Free Set)</span></p>

A set $Z$ of integers is sum-free if for all $x, y, z \in Z$, the equation $x + y = z$ has no solutions.

</div>

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Existence of Large Sum-Free Subsets)</span></p>

Subsets Every nonempty finite set $Z$ of integers contains a sum-free subset $T$ of size $\lvert T\rvert > \frac{\lvert Z\rvert}{3}$.

</div>

**Proof.**

1. Let $Z = \lbrace z_1 < \dots < z_n\rbrace$ be a set of $n$ integers.
2. Choose a prime $q = 3k + 2$ such that $q$ is larger than the absolute values of all elements in $Z$ (ensuring $q$ does not divide any $z \in Z$).
3. Let $N_q$ = $\lbrace 1, \dots, q-1\rbrace$. Pick $r \in N_q$ uniformly at random.
4. For each $z_i \in Z$, define $d_i \equiv z_i \cdot r \pmod{q}$, where $d_i \in N_q$. Since $q$ is prime, the mapping $r \mapsto z \cdot r$ is a bijection, making each $d_i$ uniformly distributed in $N_q$.
5. Define a sum-free set $M = \lbrace k+1, \dots, 2k+1\rbrace$ in the range of residues. $M$ is sum-free because the sum of the two smallest elements is $(k+1) + (k+1) = 2k+2$, which is outside the set.
6. Let $T_r = \lbrace z_i : d_i \in M\rbrace$. $T_r$ is sum-free because if $z_{i_1} + z_{i_2} = z_{i_3}$, then $d_{i_1} + d_{i_2} \equiv d_{i_3} \pmod{q}$, which cannot happen for elements in $M$.
7. We calculate the expected size of $T_r$: $\mathbb{E}[\lvert T_r\rvert] = \sum_{z_i \in Z} \mathbb{P}[d_i \in M] = \lvert Z\rvert \cdot \frac{\lvert M\rvert }{\lvert N_q\rvert} = \lvert Z\rvert \cdot \frac{k+1}{3k+1} > \frac{\lvert Z\rvert}{3}$
8. Since the expectation is greater than $\frac{\lvert Z\rvert}{3}$, there must exist at least one $r$ such that $\lvert T_r\rvert > \frac{\lvert Z\rvert}{3}$.


## The Isolating Lemma

The Isolating Lemma is a tool used to ensure that a collection of sets has a unique set with a minimum total weight.

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Weight Function)</span></p>

A function $w: X \to \mathbb{R}$. For a subset $S \subseteq X$, the weight is extended as $w(S) = \sum_{a \in S} w(a)$.

</div>

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Isolating Lemma)</span></p>

Isolating Lemma Let $X = \lbrace a_1, \dots, a_m\rbrace$ be a set and $\mathcal{F}$ be a nonempty collection of subsets of $X$. If weights $w(x)$ are chosen independently and uniformly from $\lbrace 1, \dots, 2m\rbrace$ for each $x \in X$, then with probability at least $1/2$, the minimal weight $w_{min} = \min_{S \in \mathcal{F}}w(S)$ is attained by a unique set in $\mathcal{F}$.

</div>

**Proof.**

1. A set $S \in\mathcal{F}$ is a minimum weight set if $w(S) = w_{min}$.
2. An element $a \in X$ is ambiguous if there exist two minimum weight sets $S_-$ and $S_+$ such that $a \notin S_-$ and $a \in S_+$.
3. A unique minimum weight set exists if and only if no member of $X$ is ambiguous.
4. Fix $a \in X$. Let $\mathcal{F}_- = \lbrace S \in \mathcal{F} : a \notin S\rbrace$ and $\mathcal{F}_+ = \lbrace S \in \mathcal{F} : a \in S\rbrace$.
5. Fix weights for all elements in $X \setminus \lbrace a\rbrace$. Let:
  * $w_- = \min_{S \in \mathcal{F}_-}\sum_{x \in S} w(x)$
  * $w_+ = \min_{S \in \mathcal{F}_+}\sum_{x \in S \setminus \lbrace a\rbrace} w(x)$
6. Let $d = w_- - w_+$. The total minimum weight is $w_{min} = \min(w_-, w_+ + w(a))$.
7. Both $w_-$ and $w_+ + w(a)$ are equal (making a ambiguous) if and only if $w(a) = d$.
8. Since $w(a)$ is chosen from $2m$ values independently of $d$, the probability that $w(a) = d$ is at most $1/2m$.
9. By the union bound, the probability that any element in $X$ is ambiguous is at most $m \cdot (1/2m) = 1/2$. Therefore, the probability that no element is ambiguous (and thus a unique minimum exists) is at least $1/2$.


## Crossing Numbers in Graph Theory

### Planar Graphs and Euler's Formula

A graph is planar if it can be embedded in a plane such that no edges cross. We assume standard properties of embeddings, such as the concept of faces.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Euler's Formula)</span></p>

Let $G$ be a connected planar graph with $n$ nodes, $m$ edges, and $f$ faces. Then: $f - m + n = 2$

</div>

Sketch of Proof: Use induction on the number of cycles. A tree ($0$ cycles) has $m = n-1$ and $f=1$, satisfying the formula. Removing an edge from a cycle in a graph with $k$ cycles reduces both $m$ and $f$ by $1$, maintaining the equality.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Edge Bound for Planar Graphs)</span></p>

For a planar graph $G$ with $n$ nodes and $m > 1$ edges: $m \le 3n - 6$

</div>

**Proof.** For a simple graph where $m \ge 3$, every face is bounded by at least $3$ edge sides. Thus $3f \le 2m$. Substituting $f = m - n + 2$ from Euler's formula: $3(m - n + 2) \le 2m \implies 3m - 3n + 6 \le 2m \implies m \le 3n - 6$

### Defining Crossing Numbers

<div class="math-callout math-callout--definition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Definition</span><span class="math-callout__name">(Crossing Number)</span></p>

The crossing number $\operatorname{cr}(G)$ is the minimum number of crossings in any embedding of a graph $G$ into the plane.

</div>

A minimum embedding (one with $\text{cr}(G)$ crossings) has specific properties:

1. No edge crosses itself.
2. Edges incident to a common node do not cross.
3. Two edges do not cross more than once.
4. Edges do not cross at an endpoint.

If any of these occur, the embedding can be redrawn with fewer crossings.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Basic Lower Bound)</span></p>

For any graph $G$, $\text{cr}(G) \ge m - 3n + 6$.

</div>

**Proof.** Replace every crossing in a minimum embedding with a new node. The new graph $G'$ is planar and simple, with $n' = n + \text{cr}(G)$ nodes and $m' = m + 2\text{cr}(G)$ edges (since each crossing splits two edges into four segments). Applying the planar edge bound: $m + 2cr(G) \le 3(n + \text{cr}(G)) - 6 \implies \text{cr}(G) \ge m - 3n + 6$

### The Crossing Number Theorem

For dense graphs, we can establish a much stronger lower bound.

<div class="math-callout math-callout--theorem" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Theorem</span><span class="math-callout__name">(Crossing Number Bound)</span></p>

Let $G$ be a graph with $n$ nodes and $m$ edges. If $m \ge 4n$, then: $\text{cr}(G) \ge \frac{1}{64} \frac{m^3}{n^2}$

</div>

**Proof.**

1. Let $p \in (0, 1]$ be a probability. Create a subgraph $G_p$ by picking each node of $G$ independently with probability $p$.
2. Let $n_p$, $m_p$, and $x_p$ be the number of nodes, edges, and crossings in $G_p$ (restricted from a minimum embedding of $G$).
3. From the previous proposition, we know $\mathbb{E}[x_p - m_p + 3n_p] \ge 0$.
4. Calculate expectations:
  * $\mathbb{E}[n_p] = pn$
  * $\mathbb{E}[m_p] = p^2m$
  * $\mathbb{E}[x_p] = p^4cr(G)$ (a crossing remains if all 4 endpoints of the two crossing edges are selected).
5. By linearity of expectation: $p^4cr(G) - p^2m + 3pn \ge 0$, which simplifies to: $\text{cr}(G) \ge \frac{p^2m - 3pn}{p^4}$
6. Set $p = \frac{4n}{m}$. Since $m \ge 4n, p \le 1$.
7. Substituting $p$: $\text{cr}(G) \ge \frac{(\frac{4n}{m})m - 3n}{p^3} = \frac{4n - 3n}{p^3} = \frac{n}{(\frac{4n}{m})^3} = \frac{1}{64} \frac{m^3}{n^2}$


## Geometric Probabilities: Points on a Circle

Consider the probability of a specific geometric configuration when points are placed randomly on a circle.

<div class="math-callout math-callout--proposition" markdown="1">
  <p class="math-callout__title"><span class="math-callout__label">Proposition</span><span class="math-callout__name">(Center in Convex Hull)</span></p>

If $n$ points are chosen uniformly and independently on a circle, the probability that their convex hull contains the center of the circle is $1 - \frac{2n}{2^n}$.

</div>

**Proof.**

1. Perform the experiment in two steps:
  * (i) Pick $n$ points independently.
  * (ii) For each point, flip a coin to decide whether to use that point or its mirror point (the point diametrically opposite).
2. This results in $2^n$ possible "candidate sets" of points.
3. The center of the circle is not in the convex hull if and only if all points lie in a single half-plane (the set is "one-sided").
4. There are exactly $2n$ candidate sets that are one-sided.
5. Therefore, the probability of being one-sided is $\frac{2n}{2^n}$. The probability that the center is contained in the hull is the complement: $1 - \frac{2n}{2^n}$.
