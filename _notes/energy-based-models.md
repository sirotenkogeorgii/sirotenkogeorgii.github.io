---
layout: default
title: Energy-Based Models
date: 2024-10-20
excerpt: ...
tags:
  - modeling
  - statistical-physics
  - artificial-intelligence
---

# Energy-Based Models

## Training Methods

* How is the expected log-likelihood of a general EBM derived? Where does sampling come into play?
* How does contrastive divergence approximate likelihood maximization?
* How does score matching avoid sampling?
* What problems come up with basic score matching and how are they addressed?
* In what way are contrastive divergence and score matching analogous?

### Papers on Training Methods
- [On Contrastive Divergence Learning (2005)]
- [Estimation of Non-Normalized Statistical Models by Score Matching (2005)]
- [A Connection Between Score Matching and Denoising Autoencoders (2011)]
- [Interpretation and Generalization of Score Matching (2012)]
- [Sliced Score Matching: A Scalable Approach to Density and Score Estimation (2019)]

## Other

- [Training Products of Experts by Minimizing Contrastive Divergence (2002)](/paper-notes/energy-based-models/hinton-2002/)
- [A Tutorial on Energy-Based Learning (2006)]
- [A Fast Learning Algorithm for Deep Belief Networks (2006)]
- [Deep Boltzmann Machines (2009)]
- [Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models (2010)]
- [A Theory of Generative ConvNet (2016)]
- [Implicit Generation and Generalization in Energy-Based Models (2019)]
- [Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One (2020)]
- [How to Train Your Energy-Based Models (2021)]
- [Introduction to Latent Variable Energy-Based Models: A Path Towards Autonomous Machine Intelligence (2023)]